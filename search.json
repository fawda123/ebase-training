[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ecosystem Metabolism Workshop",
    "section": "",
    "text": "About this Workshop\nThis workshop will provide an overview of techniques to estimate metabolic processes in estuaries using new software with the R open-source programming language. We will describe the theory and application of the new Estuarine BAyesian Single-station Estimation (EBASE) method that applies a Bayesian framework to a simple process-based model and dissolved oxygen observations, allowing the estimation of critical metabolic parameters as informed by a set of prior distributions. We will also explore how EBASE and additional R features can be used to evaluate metabolism results and to identify potential drivers of change. By the end of the workshop, you will understand how to prepare data for use with EBASE and to interpret the results to better understand processes that influence ecosystem status and condition. Basic R experience is expected.\nEBASE materials:",
    "crumbs": [
      "About this Workshop"
    ]
  },
  {
    "objectID": "index.html#agenda",
    "href": "index.html#agenda",
    "title": "Ecosystem Metabolism Workshop",
    "section": "Agenda",
    "text": "Agenda\nAll times EDT.\n\n\n\n\n\nTime\nTopic\n\n\n\n\n10:00\nIntroduction\n\n\n10:30\nData Preparation\n\n\n11:30\nBreak\n\n\n12:00\nUsing EBASE\n\n\n1:30\nInterpreting Results\n\n\n3:00\nAdjourn",
    "crumbs": [
      "About this Workshop"
    ]
  },
  {
    "objectID": "index.html#important-links",
    "href": "index.html#important-links",
    "title": "Ecosystem Metabolism Workshop",
    "section": "Important links",
    "text": "Important links\n\nZoom login (email the instructor for the passcode): link\nLive coding: link\nPosit Cloud: link\nEBASE package website: link\nEBASE paper: link",
    "crumbs": [
      "About this Workshop"
    ]
  },
  {
    "objectID": "index.html#housekeeping",
    "href": "index.html#housekeeping",
    "title": "Ecosystem Metabolism Workshop",
    "section": "Housekeeping",
    "text": "Housekeeping\nPlease read these housekeeping items on the day of the training so that everything runs smoothly.\n\nFeel free to verbally ask questions during the training by unmuting your microphone. You can also type questions in the chat. Other attendees are welcome to respond to questions in the chat.\n\nPlease use RStudio installed on your computer to follow along during the workshop. RStudio Cloud can also be used as a backup option. See the setup instructions below for more information.\nWe have a live coding link that we’ll be using as we go through the lessons. If you get lost, you can copy/paste code from this link into RStudio.\nAll training content is on this website. If you get lost you can view the agenda to see which lesson we’re covering.",
    "crumbs": [
      "About this Workshop"
    ]
  },
  {
    "objectID": "index.html#setup",
    "href": "index.html#setup",
    "title": "Ecosystem Metabolism Workshop",
    "section": "Setup",
    "text": "Setup\nPlease visit the setup page for instructions on preparing for this workshop. You will be required to install R, RStudio, JAGS, and several R packages prior to the workshop. Basic R experience is expected.",
    "crumbs": [
      "About this Workshop"
    ]
  },
  {
    "objectID": "index.html#code-of-conduct",
    "href": "index.html#code-of-conduct",
    "title": "Ecosystem Metabolism Workshop",
    "section": "Code of Conduct",
    "text": "Code of Conduct\nWe are dedicated to providing a welcoming and supportive environment for all people, regardless of background or identity. We are adopting The Carpentries Code of Conduct for this workshop.",
    "crumbs": [
      "About this Workshop"
    ]
  },
  {
    "objectID": "index.html#feedback",
    "href": "index.html#feedback",
    "title": "Ecosystem Metabolism Workshop",
    "section": "Feedback",
    "text": "Feedback\nEBASE is a tool for you and we sincerely appreciate any feedback on ways to improve its functionality to serve your needs. We encourage you to contact the instructor with any feedback or suggestions. Alternatively, issues can be posted on the main GitHub page (requires a GitHub account).",
    "crumbs": [
      "About this Workshop"
    ]
  },
  {
    "objectID": "index.html#conveners",
    "href": "index.html#conveners",
    "title": "Ecosystem Metabolism Workshop",
    "section": "Conveners",
    "text": "Conveners\n\nMarcus Beck, Ph.D.\n\n\n\n\nMarcus Beck is the Program Scientist for the Tampa Bay Estuary Program in St. Petersburg, Florida and is developing data analysis and visualization methods for Bay health indicators. Marcus has experience researching environmental indicators and developing open science products to support environmental decision-making. He has been using the R statistical programming language for over 15 years and has taught several workshops on its application to environmental sciences. Marcus has also developed several R packages and currently maintains 7 on CRAN. He received a PhD in Conservation Biology with a minor in Statistics from the University of Minnesota in 2013, his Masters in Conservation Biology from the University of Minnesota in 2009, and his Bachelors in Zoology from the University of Florida in 2007. Links: Email, CV, GitHub, Scholar\n\n\n\n\nJill Arriola, Ph.D.\n\n\n\n\nJill Arriola will soon be a staff scientist with the Alliance for Aquatic Resource Monitoring (ALLARM) at Dickinson College in Carlisle, Pennsylvania. Jill has over 15 years of experience as an estuary scientist, primarily in tidal wetland biogeochemistry. She was recently a Postdoctoral Scholar at Pennsylvania State University in the Department of Meteorology and Atmospheric Science. She received her Ph.D. in Marine Science from the University of North Carolina in 2019 and a Bachelors in Environmental and Ocean Science from the University of Massachusetts in 2012.",
    "crumbs": [
      "About this Workshop"
    ]
  },
  {
    "objectID": "01_intro.html",
    "href": "01_intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Lesson Outline\nThis lesson will cover the very basics of ecosystem metabolism: what it is, why you should care about it, and what it can tell you. We’ll also make sure that RStudio is setup for the rest of the workshop.",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01_intro.html#learning-goals",
    "href": "01_intro.html#learning-goals",
    "title": "1  Introduction",
    "section": "1.2 Learning Goals",
    "text": "1.2 Learning Goals\n\nLearn a basic definition of metabolism\nUnderstand what it tells you about your ecosystem\nUnderstand how it is measured\nSetup RStudio for the workshop using a project",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01_intro.html#what-is-ecosystem-metabolism",
    "href": "01_intro.html#what-is-ecosystem-metabolism",
    "title": "1  Introduction",
    "section": "1.3 What is ecosystem metabolism?",
    "text": "1.3 What is ecosystem metabolism?\nSimply put, ecosystem metabolism is a measure of how quickly organic matter is consumed or produced in an aquatic environment. Estuaries are transition zones that receive materials from the land and export them to the ocean and atmosphere. Ecosystem metabolism is a measure of the rate at which this material is processed as a first order property that can affect a wide range of biogeochemical processes.\nMore specifically, ecosystem metabolism is the balance between production and respiration processes that create and consume organic matter. We can use these measures to characterize metabolism in aquatic environments as a rate, as compared to “surrogate” snapshot measures like nutrient or chlorophyll concentrations. It provides a more complete picture of the state of the environment.\nThe simplest expression of metabolism is below:\n\\[\nNEM = P - R\n\\]\nHere, net ecosystem metabolism (\\(NEM\\)) is the difference between gross primary production (\\(P\\)) and respiration (\\(R\\)). Or, net ecosystem metabolism is the difference between processes that produce organic matter and those that consume organic matter. You may also see \\(NEM\\) written as net ecosystem production (\\(NEP\\)).\n\\(NEM\\) can tell us a lot about an ecosystem:\n\nIf positive, the ecosystem is autotrophic, meaning it produces more organic matter than it consumes. Organic matter will accumulate.\nIf negative, the ecosystem is heterotrophic, meaning it consumes more organic matter than it produces. Organic matter will deplete.\n\nAutotrophy and heterotrophy can be governed by many factors. For example, autotrophy may occur more often during warmer months, as temperature can stimulate growth of algae and other primary primary producers. Similarly, hyper-eutrophic systems may be more heterotrophic during the summer as the rapid decomposition of organic matter that was produced by excess nutrients may be increased with temperature. A single pulse of nutrients can create autotrophic conditions as production increases followed by heterotrophic conditions as the new organic material settles to the bottom and is decomposed.\nThis graphic from a synthesis study of 350 sites by Hoellein et al. (2013) shows how metabolic ranges vary across different aquatic ecosystems. Here we can see that the largest ranges in production and respiration occur in estuaries and most aquatic systems tend towards heterotrophy. Note the units as grams of oxygen per m\\(^{-2}\\) per day.\n\n\n\n\n\nMost of the estuarine sites used in Hoellein et al. (2013) were from a single study by Caffrey (2004) that evaluated metabolism across all sites in the National Estuarine Research Reserve (NERR). This study was one of the first to develop a system-wide comparison of metabolic rates and several useful conclusions were made.\nFor example, metabolic rates between estuarine habitats are shown below, where most sites tend towards heterotrophy, although sites dominated by submerged aquatic vegetation are more balanced or even autotrophic.\n\n\n\n\n\nNutrients as drivers of metabolism were also evaluated by Caffrey (2004). Although the relationships were weak, sites with higher nutrient loadings tended towards autotrophy as production was generally higher than respiration.\n\n\n\n\n\nMetabolism is also temporally variable. In addition to seasonal variation, metabolism can be event-driven and respond to external and internal drivers that influence the system. A severe red tide bloom in Tampa Bay occurred in July 2021. The dissolved oxygen time series shows a clear increase and decrease as the bloom developed and then dissipated.\n\n\n\n\n\nThe change in ecosystem metabolism from autototrophy to heterotrophy tells a story about the bloom. We see an increase in production followed by an increase in respiration with bloom development and senescence. Also note the “anomalous” values for production and respiration that are negative and positive, respectively.\n\n\n\n\n\nThese quick examples demonstrate that unique information about system dynamics can be inferred from ecosystem metabolism and its components. As you learn to assess metabolism with the tools from this workshop, you can develop similar hypotheses on factors that control metabolism both within and between systems.",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01_intro.html#how-is-metabolism-measured",
    "href": "01_intro.html#how-is-metabolism-measured",
    "title": "1  Introduction",
    "section": "1.4 How is metabolism measured?",
    "text": "1.4 How is metabolism measured?\nMetabolism can be measured several ways, each of which has its own strengths, weaknesses, and assumptions (Kemp and Testa 2011). These include:\n\nBottle-based incubations\nOpen-water techniques\nEcosystem budgets\nUse of oxygen isotopes or inert gases\nAquatic eddy covariance\n\nOpen-water techniques are among the more commonly used approaches applied to continuous monitoring data. These techniques exploit the diel cycle of dissolved oxygen (DO) concentration to infer metabolism. The main assumptions are:\n\nProduction produces oxygen during daylight hours\nRespiration consumes oxygen during daylight and nighttime hours\nOxygen exchanges freely with the atmosphere\nThe system is well-mixed\n\n\nThe DO time series can be deconstructed using a mass-balance equation to estimate metabolism. This is a literal deconstruction of the processes in the above figure.\n\\[\nZ\\frac{dC_d}{dt} = P - R + D\n\\]\nThe change in dissolved oxygen (\\(dC_d\\)) over time (\\(dt\\)) (a rate) is equal to the difference between production (\\(P\\)) and respiration (\\(R\\)) plus the exchange of oxygen with the atmosphere (\\(D\\)). The units are converted from volumetric (\\(m^{-3}\\)) to areal (\\(m^{-2}\\)) by multiplying the equation by water column depth (\\(Z\\)). In this example, \\(D\\) is positive for ingassing and negative for outgassing.\n\\(P\\), \\(R\\), and \\(D\\) can be estimated several ways depending on which “open-water” technique is used. The most popular method is that of Odum (1956), modified extensively by others since its original presentation. The method calculates the DO flux (DO change per unit time) during day and night periods and corrects it for gas exchange. This is also called the book-keeping method since it’s “simple” arithmetic.\nGas exchange can also be estimated several ways, but the core concept is that the flux in or out of the water is proportional to the difference between the DO saturation concentration (\\(C_s\\)) and the measured concentration in the water (\\(C_d\\)). This difference is also multiplied by a gas exchange coefficient (\\(k\\)), which can vary by wind, temperature, or other factors depending on the model.\n\\[\nD = k(C_s - C_d)\n\\]\nFortunately for us, there are existing packages in R that can estimate these parameters. One is the WtRegDO package that uses the Odum technique (Beck et al. 2015), which was covered in a workshop a few years ago. Another is EBASE, which we’ll discuss today. EBASE follows the same general principles as the Odum technique, but differs in the statistical method to estimate the parameter (i.e., Bayesian) and the model used for each parameter (Beck et al. 2024).\nWe’ll talk more about how EBASE estimates metabolism in Lesson 3, but first we need to get setup with R and prepare our data in Lesson 2.",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01_intro.html#get-ready-for-the-workshop",
    "href": "01_intro.html#get-ready-for-the-workshop",
    "title": "1  Introduction",
    "section": "1.5 Get ready for the workshop",
    "text": "1.5 Get ready for the workshop\nLet’s make sure RStudio is setup for the workshop. We’ll use a project to keep everything in one place, for good reason. There are people that will set your computer on fire if you don’t follow these best practices.\nLet’s run through the basics of creating a project in RStudio.\n\nCreate a new project in RStudio, first open RStudio and select “New project” from the File menu at the top.\n\nThen select “New Directory”.\n\nThen select “New Project”. Create a directory in a location that’s easy to find.\n\nClick “Create Project” when you’re done.\nA fresh RStudio session will open. Open a new R script by selecting “New file” &gt; “R Script” from the File menu at the top.\n\nSave the file in your working directory by clicking the file icon on the top right. Give it an informative name. For example, “ebase_intro.R”. The file should be saved to the project root or home directory.\n\nSetup the package imports at the top of the script. These should already be installed during the setup prior to the workshop.\nlibrary(EBASE)\nlibrary(SWMPr)\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(here)\nlibrary(doParallel)\nSave the script again and send the commands from the source script to the console. Do they load properly??\nSetup a data folder in the project directory. This is where we’ll store the data for the workshop.\nSelect “New folder” from the file pane (bottom right). \nName the folder “data” and save it in the project root directory.\nDownload the zipped data file from GitHub: https://github.com/fawda123/ebase-training/raw/main/data/367272.zip\nPlace the downloaded data in this folder. There’s no need to unzip it.",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01_intro.html#next-steps",
    "href": "01_intro.html#next-steps",
    "title": "1  Introduction",
    "section": "1.6 Next steps",
    "text": "1.6 Next steps\nIn this lesson we learned the basics about metabolism and made sure RStudio is ready to go for the rest of the workshop. Next we’ll learn how to import and prepare data with SWMPr and other packages for analysis with EBASE.\n\n\n\n\nBeck, M. W., J. M. Arriola, M. Herrmann, and R. G. Najjar. 2024. Fitting metabolic models to dissolved oxygen data: The estuarine bayesian single-station estimation method. Limnology and Oceanography: Methods 22: 590–607. doi:10.1002/lom3.10620\n\n\nBeck, M. W., J. D. Hagy III, and M. C. Murrell. 2015. Improving estimates of ecosystem metabolism by reducing effects of tidal advection on dissolved oxygen time series. Limnology and Oceanography: Methods 13: 731–745. doi:10.1002/lom3.10062\n\n\nCaffrey, J. M. 2004. Factors controlling net ecosystem metabolism in U.S. estuaries. Estuaries 27: 90–101. doi:10.1007/bf02803563\n\n\nHoellein, T. J., D. A. Bruesewitz, and D. C. Richardson. 2013. Revisiting Odum (1956): A synthesis of aquatic ecosystem metabolism. Limnology and Oceanography 58: 2089–2100. doi:10.4319/lo.2013.58.6.2089\n\n\nKemp, W. M., and J. M. Testa. 2011. Metabolic balance between ecosystem production and consumption, p. 83–118. In E. Wolanski and D. McLusky [eds.], Treatise on estuarine and coastal science. Elsevier.\n\n\nOdum, H. T. 1956. Primary production in flowing waters. Limnology and Oceanography 1: 102–117.",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02_dataprep.html",
    "href": "02_dataprep.html",
    "title": "2  Data Preparation",
    "section": "",
    "text": "2.1 Lesson Outline\nThe EBASE software requires a specific format for input data that includes both water quality and weather data. This lesson will demonstrate how the SWMPr package can be used to import and prepare data from the NERRS System Wide Monitoring Program (SWMP) for analysis. We’ll also cover how to prepare non-SWMP data using R packages from the tidyverse.",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Preparation</span>"
    ]
  },
  {
    "objectID": "02_dataprep.html#learning-goals",
    "href": "02_dataprep.html#learning-goals",
    "title": "2  Data Preparation",
    "section": "2.2 Learning Goals",
    "text": "2.2 Learning Goals\n\nUnderstand the data requirements for EBASE\nLearn how to import data with SWMPr\nLearn how to clean and combine SWMP data\nLearn how to prepare data for EBASE analysis with other tools",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Preparation</span>"
    ]
  },
  {
    "objectID": "02_dataprep.html#sec-ebasedata",
    "href": "02_dataprep.html#sec-ebasedata",
    "title": "2  Data Preparation",
    "section": "2.3 Data requirements for EBASE",
    "text": "2.3 Data requirements for EBASE\nThe metabolism functions in EBASE require both water quality and weather data. Both are assumed continuous where sufficient observations are collected each day to describe the diel cycling of dissolved oxygen and other required parameters. Because water quality and weather data are collected by different sensors, these data typically are not provided in the same file. A bit of preprocessing is needed to combine the data before EBASE can be used.\n\n\n\n\n\n\nNote\n\n\n\nEBASE requires water quality and weather data.\n\n\nWe’ll start by loading EBASE and viewing an example file that’s included with the package. This example file is used for all the documentation in EBASE and you can use it to learn how the main functions work. We’ll use other data files below.\n\nlibrary(EBASE)\n\nView the first few rows of the example data file:\n\nhead(exdat)\n\n        DateTimeStamp DO_obs Temp  Sal PAR WSpd\n1 2012-02-23 00:00:00    8.8 16.4 23.0   0  3.6\n2 2012-02-23 00:15:00    8.8 16.4 22.8   0  3.5\n3 2012-02-23 00:30:00    8.8 16.4 22.7   0  3.6\n4 2012-02-23 00:45:00    8.8 16.4 22.9   0  4.2\n5 2012-02-23 01:00:00    8.7 16.4 22.7   0  3.6\n6 2012-02-23 01:15:00    8.5 16.4 23.4   0  4.1\n\n\nCheck the structure of the example data file:\n\nstr(exdat)\n\n'data.frame':   27648 obs. of  6 variables:\n $ DateTimeStamp: POSIXct, format: \"2012-02-23 00:00:00\" \"2012-02-23 00:15:00\" ...\n $ DO_obs       : num  8.8 8.8 8.8 8.8 8.7 8.5 8.3 8.2 7.8 8.5 ...\n $ Temp         : num  16.4 16.4 16.4 16.4 16.4 16.4 16.4 16.4 16.4 16.4 ...\n $ Sal          : num  23 22.8 22.7 22.9 22.7 23.4 24.5 24.4 24.3 23 ...\n $ PAR          : num  0 0 0 0 0 0 0 0 0 0 ...\n $ WSpd         : num  3.6 3.5 3.6 4.2 3.6 4.1 3.6 4.2 4 3.6 ...\n\n\nOnly five parameters are required to use EBASE, where each row represents a single observation indexed by ascending time.\n\nDateTimeStamp: Date and time of the observation, as a POSIXct object with an appropriate time zone.\nDO_obs: Observed dissolved oxygen concentration in mg/L.\nTemp: Water temperature in degrees Celsius.\nSal: Salinity in psu.\nPAR: Total photosynthetically active radiation in Watts per square meter.\nWSpd: Wind speed in meters per second.\n\nThe dissolved oxygen, water temperature, and salinity data are typically collected by a water quality sonde, while the PAR and wind speed data are typically collected at a weather station. We’ll discuss why these parameters are needed to calculate metabolism in Lesson 3. These data must be combined into a single data frame to work with EBASE. We’ll do this in the next few sections.",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Preparation</span>"
    ]
  },
  {
    "objectID": "02_dataprep.html#data-import-with-swmpr",
    "href": "02_dataprep.html#data-import-with-swmpr",
    "title": "2  Data Preparation",
    "section": "2.4 Data import with SWMPr",
    "text": "2.4 Data import with SWMPr\nThe SWMPr package was developed in 2016 to provide a bridge between the raw data from the NERR System Wide Monitoring Program (SWMP) and R (Beck 2016). It does this with some degree of proficiency, but by far it’s most useful feature is being able to import and combine time series from SWMP sites with relative ease. In particular, the import_local(), qaqc(), and comb() functions allow us to quickly import, clean up, and combine datasets for follow-up analysis. This is all we’ll do with SWMPr in these lessons.\nThe import_local() function is designed to work with SWMP data downloaded from the Centralized Data Management Office (CDMO) using the Zip Downloads feature from “zip downloads” feature of the Advanced Query System. The files we have in our “data” folder were requested from this feature for Apalachicola Bay data from 2017 to 2019.\n\n\n\n\n\n\nNote\n\n\n\nUse import_local() to import SWMP data downloaded from the Advanced Query System.\n\n\nThe import_local() function has two arguments: path to indicate where the data are located and station_code to indicate which station to import. We first load SWMPr and then use the function to import data for the Apalachicola Dry Bar station:\n\n# load SWMPr\nlibrary(SWMPr)\n\n# import data\napadbwq &lt;- import_local(path = 'data/367272.zip', station_code = 'apadbwq')\n\n# characteristics of the dataset\nhead(apadbwq)\n\n        datetimestamp temp f_temp spcond f_spcond  sal f_sal do_pct f_do_pct\n1 2017-01-01 00:00:00 15.9    &lt;0&gt;  34.99      &lt;0&gt; 22.1   &lt;0&gt;  104.1      &lt;0&gt;\n2 2017-01-01 00:15:00 15.9    &lt;0&gt;  34.94      &lt;0&gt; 22.0   &lt;0&gt;  104.0      &lt;0&gt;\n3 2017-01-01 00:30:00 15.9    &lt;0&gt;  34.90      &lt;0&gt; 22.0   &lt;0&gt;  103.5      &lt;0&gt;\n4 2017-01-01 00:45:00 15.9    &lt;0&gt;  34.94      &lt;0&gt; 22.0   &lt;0&gt;  103.3      &lt;0&gt;\n5 2017-01-01 01:00:00 15.9    &lt;0&gt;  34.99      &lt;0&gt; 22.1   &lt;0&gt;  104.1      &lt;0&gt;\n6 2017-01-01 01:15:00 15.9    &lt;0&gt;  35.08      &lt;0&gt; 22.1   &lt;0&gt;  103.9      &lt;0&gt;\n  do_mgl f_do_mgl depth f_depth cdepth f_cdepth level f_level clevel f_clevel\n1    9.0      &lt;0&gt;  1.88     &lt;0&gt;   1.81      &lt;3&gt;    NA    &lt;-1&gt;     NA     &lt;NA&gt;\n2    9.0      &lt;0&gt;  1.88     &lt;0&gt;   1.81      &lt;3&gt;    NA    &lt;-1&gt;     NA     &lt;NA&gt;\n3    9.0      &lt;0&gt;  1.89     &lt;0&gt;   1.82      &lt;3&gt;    NA    &lt;-1&gt;     NA     &lt;NA&gt;\n4    8.9      &lt;0&gt;  1.89     &lt;0&gt;   1.83      &lt;3&gt;    NA    &lt;-1&gt;     NA     &lt;NA&gt;\n5    9.0      &lt;0&gt;  1.90     &lt;0&gt;   1.84      &lt;3&gt;    NA    &lt;-1&gt;     NA     &lt;NA&gt;\n6    9.0      &lt;0&gt;  1.90     &lt;0&gt;   1.84      &lt;3&gt;    NA    &lt;-1&gt;     NA     &lt;NA&gt;\n   ph f_ph turb f_turb chlfluor f_chlfluor\n1 8.3  &lt;0&gt;   15    &lt;0&gt;       NA       &lt;-1&gt;\n2 8.2  &lt;0&gt;   15    &lt;0&gt;       NA       &lt;-1&gt;\n3 8.3  &lt;0&gt;   13    &lt;0&gt;       NA       &lt;-1&gt;\n4 8.3  &lt;0&gt;   16    &lt;0&gt;       NA       &lt;-1&gt;\n5 8.3  &lt;0&gt;   10    &lt;0&gt;       NA       &lt;-1&gt;\n6 8.3  &lt;0&gt;   14    &lt;0&gt;       NA       &lt;-1&gt;\n\ndim(apadbwq)\n\n[1] 105120     25\n\nrange(apadbwq$datetimestamp)\n\n[1] \"2017-01-01 00:00:00 EST\" \"2019-12-31 23:45:00 EST\"\n\n\nNote that this function was able to import and combine data from multiple csv files. We would have had to do this by hand if we were importing data with more general import functions available in R (e.g., read.csv).",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Preparation</span>"
    ]
  },
  {
    "objectID": "02_dataprep.html#cleaning-and-combining-swmp-data",
    "href": "02_dataprep.html#cleaning-and-combining-swmp-data",
    "title": "2  Data Preparation",
    "section": "2.5 Cleaning and combining SWMP data",
    "text": "2.5 Cleaning and combining SWMP data\nEach row has data for multiple parameters at 15 minute intervals. Each parameter also includes a column with QAQC flags, i.e., f_ then the parameter name. We can use the qaqc() function to “screen” observations with specific QAQC flags. We’ll keep all observations that have the flags 0, 1, 2, 3, 4, and 5 by indicating this information in the qaqc_keep argument (in practice, you may only want to keep data with a “zero” flag). You can also view a tabular summary of the flags in a dataset using the qaqcchk function.\n\n# keep only observations that passed qaqc chekcs\napadbwq &lt;- qaqc(apadbwq, qaqc_keep = c('0', '1', '2', '3', '4', '5'))\n\n# check the results\nhead(apadbwq)\n\n        datetimestamp temp spcond  sal do_pct do_mgl depth cdepth level clevel\n1 2017-01-01 00:00:00 15.9  34.99 22.1  104.1    9.0  1.88   1.81    NA     NA\n2 2017-01-01 00:15:00 15.9  34.94 22.0  104.0    9.0  1.88   1.81    NA     NA\n3 2017-01-01 00:30:00 15.9  34.90 22.0  103.5    9.0  1.89   1.82    NA     NA\n4 2017-01-01 00:45:00 15.9  34.94 22.0  103.3    8.9  1.89   1.83    NA     NA\n5 2017-01-01 01:00:00 15.9  34.99 22.1  104.1    9.0  1.90   1.84    NA     NA\n6 2017-01-01 01:15:00 15.9  35.08 22.1  103.9    9.0  1.90   1.84    NA     NA\n   ph turb chlfluor\n1 8.3   15       NA\n2 8.2   15       NA\n3 8.3   13       NA\n4 8.3   16       NA\n5 8.3   10       NA\n6 8.3   14       NA\n\ndim(apadbwq)\n\n[1] 105120     13\n\nrange(apadbwq$datetimestamp)\n\n[1] \"2017-01-01 00:00:00 EST\" \"2019-12-31 23:45:00 EST\"\n\n\nNotice that the number of rows are the same as before - no rows are removed by qaqc(). Values that did not fit the screening criteria are given a NA value. Also notice the flag columns are removed.\n\n\n\n\n\n\nNote\n\n\n\nUse the qaqc() function to remove observations with specific QAQC flags.\n\n\nThe EBASE functions also require weather data. We can repeat the steps above to import and clean data from the weather station at Apalachicola.\n\n# import weather data, clean it up\napaebmet &lt;- import_local(path = 'data/367272.zip', station_code = 'apaebmet')\napaebmet &lt;- qaqc(apaebmet, qaqc_keep = c('0', '1', '2', '3', '4', '5'))\n\n# check the results\nhead(apaebmet)\n\n        datetimestamp atemp rh   bp wspd maxwspd wdir sdwdir totpar totprcp\n1 2017-01-01 00:00:00  15.7 89 1020  4.8     6.7   97     10      0       0\n2 2017-01-01 00:15:00  15.8 90 1020  4.9     6.5  103      9      0       0\n3 2017-01-01 00:30:00  16.1 91 1020  3.8     5.8  107     11      0       0\n4 2017-01-01 00:45:00  16.5 92 1019  3.3     6.4  119     15      0       0\n5 2017-01-01 01:00:00  16.9 93 1019  5.3     8.6  113      9      0       0\n6 2017-01-01 01:15:00  17.3 93 1019  3.9     5.6  118     10      0       0\n  totsorad\n1       NA\n2       NA\n3       NA\n4       NA\n5       NA\n6       NA\n\ndim(apaebmet)\n\n[1] 105120     11\n\nrange(apaebmet$datetimestamp)\n\n[1] \"2017-01-01 00:00:00 EST\" \"2019-12-31 23:45:00 EST\"\n\n\nThe comb() function in SWMPr lets us combine data from two locations using the datetimestamp column. We need to do this to use the functions in EBASE that require both water quality and weather data.\n\n\n\n\n\n\nNote\n\n\n\nThe comb() function combines water quality and weather data.\n\n\nThere are a couple of arguments to consider for the comb() function. First, the timestep argument defines the time step for the resulting output. Keep this at 15 to retain all of the data. You could use a larger time step to subset the data if, for example, we wanted data every 60 minutes. Second, the method argument defines how two datasets with different date ranges are combined. Use method = 'union' to retain the entire date range across both datasets or use method = 'intersect' to retain only the dates that include data from both datasets. For our example, union and intersect produce the same results since the date ranges and time steps are the same.\nTo speed up the examples in our lesson, we’ll use a 60 minute timestep. In practice, it’s better to retain all of the data (i.e., timestep = 15).\n\n# combine water quality and weather data\napadb &lt;- comb(apadbwq, apaebmet, timestep = 60, method = 'union')\n\n# check the results\nhead(apadb)\n\n        datetimestamp temp spcond  sal do_pct do_mgl depth cdepth level clevel\n1 2017-01-01 00:00:00 15.9  34.99 22.1  104.1    9.0  1.88   1.81    NA     NA\n2 2017-01-01 01:00:00 15.9  34.99 22.1  104.1    9.0  1.90   1.84    NA     NA\n3 2017-01-01 02:00:00 15.9  35.22 22.2  103.5    9.0  1.94   1.89    NA     NA\n4 2017-01-01 03:00:00 16.0  36.95 23.4  100.1    8.6  2.00   1.95    NA     NA\n5 2017-01-01 04:00:00 16.0  37.10 23.5   99.7    8.5  2.01   1.96    NA     NA\n6 2017-01-01 05:00:00 16.1  37.19 23.6   98.8    8.4  2.01   1.96    NA     NA\n   ph turb chlfluor atemp rh   bp wspd maxwspd wdir sdwdir totpar totprcp\n1 8.3   15       NA  15.7 89 1020  4.8     6.7   97     10      0       0\n2 8.3   10       NA  16.9 93 1019  5.3     8.6  113      9      0       0\n3 8.3   13       NA  18.3 94 1018  3.9     5.2  121      7      0       0\n4 8.2   22       NA  19.0 92 1018  4.6     6.3  151      8      0       0\n5 8.2   28       NA  19.5 90 1018  5.0     6.8  153      9      0       0\n6 8.2   11       NA  19.4 90 1018  4.2     6.3  151      9      0       0\n  totsorad\n1       NA\n2       NA\n3       NA\n4       NA\n5       NA\n6       NA\n\ndim(apadb)\n\n[1] 26281    23\n\nrange(apadb$datetimestamp)\n\n[1] \"2017-01-01 EST\" \"2020-01-01 EST\"\n\n\n\n\n\n\n\n\n Exercise 1\n\n\n\nRepeat the above examples but do this using data for the East Bay water quality station at Apalachicola. Import data for apaebwq and abaebmet, clean them up with qaqc(), and combine them with comb().\n\nCreate and name a section header in your script with Ctrl + Shift + R. Enter all exercise code in this section.\nLoad the SWMPr package with the library function. This should already be installed from last time (i.e., install.packages('SWMPr')).\nImport and clean up apaebwq with import_local() and qaqc().\nImport and clean up apaebmet with import_local() and qaqc().\nCombine the two with comb(). Use a 60 minute time step and use the union option.\n\n\n\n\n\n\n\n Answers\n\n\n\n\n\n\n# import water quality data, clean it up\napaebwq &lt;- import_local(path = 'data/367272.zip', station_code = 'apaebwq')\napaebwq &lt;- qaqc(apaebwq, qaqc_keep = c('0', '1', '2', '3', '4', '5'))\n\n# import weather data, clean it up\napaebmet &lt;- import_local(path = 'data/367272.zip', station_code = 'apaebmet')\napaebmet &lt;- qaqc(apaebmet, qaqc_keep = c('0', '1', '2', '3', '4', '5'))\n\n# combine water quality and weather data\napaeb &lt;- comb(apaebwq, apaebmet, timestep = 60, method = 'union')",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Preparation</span>"
    ]
  },
  {
    "objectID": "02_dataprep.html#preparing-swmp-data-for-ebase",
    "href": "02_dataprep.html#preparing-swmp-data-for-ebase",
    "title": "2  Data Preparation",
    "section": "2.6 Preparing SWMP data for EBASE",
    "text": "2.6 Preparing SWMP data for EBASE\nThere are a few more steps before we can use EBASE now that we’ve combined the water quality and weather data. As with most R functions, the input formats are very specific requiring us to make sure the column names, locations of the columns, and types of columns in our data are exactly as needed.\nThe example dataset from EBASE can be used to guide us in preparing the data. We can compare this to our Apalachicola dataset from above.\n\n# view first six rows of example data\nhead(exdat)\n\n        DateTimeStamp DO_obs Temp  Sal PAR WSpd\n1 2012-02-23 00:00:00    8.8 16.4 23.0   0  3.6\n2 2012-02-23 00:15:00    8.8 16.4 22.8   0  3.5\n3 2012-02-23 00:30:00    8.8 16.4 22.7   0  3.6\n4 2012-02-23 00:45:00    8.8 16.4 22.9   0  4.2\n5 2012-02-23 01:00:00    8.7 16.4 22.7   0  3.6\n6 2012-02-23 01:15:00    8.5 16.4 23.4   0  4.1\n\n# view the structure of example data\nstr(exdat)\n\n'data.frame':   27648 obs. of  6 variables:\n $ DateTimeStamp: POSIXct, format: \"2012-02-23 00:00:00\" \"2012-02-23 00:15:00\" ...\n $ DO_obs       : num  8.8 8.8 8.8 8.8 8.7 8.5 8.3 8.2 7.8 8.5 ...\n $ Temp         : num  16.4 16.4 16.4 16.4 16.4 16.4 16.4 16.4 16.4 16.4 ...\n $ Sal          : num  23 22.8 22.7 22.9 22.7 23.4 24.5 24.4 24.3 23 ...\n $ PAR          : num  0 0 0 0 0 0 0 0 0 0 ...\n $ WSpd         : num  3.6 3.5 3.6 4.2 3.6 4.1 3.6 4.2 4 3.6 ...\n\n# view first six rows of apadb data\nhead(apadb)\n\n        datetimestamp temp spcond  sal do_pct do_mgl depth cdepth level clevel\n1 2017-01-01 00:00:00 15.9  34.99 22.1  104.1    9.0  1.88   1.81    NA     NA\n2 2017-01-01 01:00:00 15.9  34.99 22.1  104.1    9.0  1.90   1.84    NA     NA\n3 2017-01-01 02:00:00 15.9  35.22 22.2  103.5    9.0  1.94   1.89    NA     NA\n4 2017-01-01 03:00:00 16.0  36.95 23.4  100.1    8.6  2.00   1.95    NA     NA\n5 2017-01-01 04:00:00 16.0  37.10 23.5   99.7    8.5  2.01   1.96    NA     NA\n6 2017-01-01 05:00:00 16.1  37.19 23.6   98.8    8.4  2.01   1.96    NA     NA\n   ph turb chlfluor atemp rh   bp wspd maxwspd wdir sdwdir totpar totprcp\n1 8.3   15       NA  15.7 89 1020  4.8     6.7   97     10      0       0\n2 8.3   10       NA  16.9 93 1019  5.3     8.6  113      9      0       0\n3 8.3   13       NA  18.3 94 1018  3.9     5.2  121      7      0       0\n4 8.2   22       NA  19.0 92 1018  4.6     6.3  151      8      0       0\n5 8.2   28       NA  19.5 90 1018  5.0     6.8  153      9      0       0\n6 8.2   11       NA  19.4 90 1018  4.2     6.3  151      9      0       0\n  totsorad\n1       NA\n2       NA\n3       NA\n4       NA\n5       NA\n6       NA\n\n# view the structure of apadb data\nstr(apadb)\n\nClasses 'swmpr' and 'data.frame':   26281 obs. of  23 variables:\n $ datetimestamp: POSIXct, format: \"2017-01-01 00:00:00\" \"2017-01-01 01:00:00\" ...\n $ temp         : num  15.9 15.9 15.9 16 16 16.1 16.2 16.2 16.1 16.3 ...\n $ spcond       : num  35 35 35.2 37 37.1 ...\n $ sal          : num  22.1 22.1 22.2 23.4 23.5 23.6 23.7 23.2 22.4 23.3 ...\n $ do_pct       : num  104.1 104.1 103.5 100.1 99.7 ...\n $ do_mgl       : num  9 9 9 8.6 8.5 8.4 8.1 8.1 8.2 7.9 ...\n $ depth        : num  1.88 1.9 1.94 2 2.01 2.01 1.95 1.89 1.81 1.74 ...\n $ cdepth       : num  1.81 1.84 1.89 1.95 1.96 1.96 1.9 1.84 1.75 1.68 ...\n $ level        : num  NA NA NA NA NA NA NA NA NA NA ...\n $ clevel       : num  NA NA NA NA NA NA NA NA NA NA ...\n $ ph           : num  8.3 8.3 8.3 8.2 8.2 8.2 8.1 8.1 8.1 8.1 ...\n $ turb         : num  15 10 13 22 28 11 23 26 26 27 ...\n $ chlfluor     : num  NA NA NA NA NA NA NA NA NA NA ...\n $ atemp        : num  15.7 16.9 18.3 19 19.5 19.4 19.3 19.7 19.7 20.3 ...\n $ rh           : num  89 93 94 92 90 90 91 89 90 90 ...\n $ bp           : num  1020 1019 1018 1018 1018 ...\n $ wspd         : num  4.8 5.3 3.9 4.6 5 4.2 4.3 5.3 4.9 5.6 ...\n $ maxwspd      : num  6.7 8.6 5.2 6.3 6.8 6.3 5.6 7.5 7.2 7.5 ...\n $ wdir         : num  97 113 121 151 153 151 147 153 155 156 ...\n $ sdwdir       : num  10 9 7 8 9 9 8 9 10 9 ...\n $ totpar       : num  0 0 0 0 0 ...\n $ totprcp      : num  0 0 0 0 0 0 0 0 0 0 ...\n $ totsorad     : num  NA NA NA NA NA NA NA NA NA NA ...\n - attr(*, \"station\")= chr [1:2] \"apadbwq\" \"apaebmet\"\n - attr(*, \"parameters\")= chr [1:22] \"temp\" \"spcond\" \"sal\" \"do_pct\" ...\n - attr(*, \"qaqc_cols\")= logi FALSE\n - attr(*, \"cens_cols\")= logi FALSE\n - attr(*, \"date_rng\")= POSIXct[1:2], format: \"2017-01-01\" \"2020-01-01\"\n - attr(*, \"timezone\")= chr \"America/Jamaica\"\n - attr(*, \"stamp_class\")= chr [1:2] \"POSIXct\" \"POSIXt\"\n\n\nSo, we need to do a few things to our Apalachicola dataset to match the format of the exdat dataset. We can use the dplyr package to “wrangle” the data into the correct format (here’s a useful cheatsheet for this package). The dplyr package comes with the tidyverse.\n\n\n\n\n\n\nNote\n\n\n\nEBASE requires specific column names (case-sensitive) and units.\n\n\nFirst we need to rename the columns and select those we want to make apadb look more like exdat. This can all be done with the select function in dplyr. Note that we’ll also retain the depth column, which is not needed for EBASE but is useful to retain.\n\n# load dplyr\nlibrary(dplyr)\n\n# select and rename columns\napadb &lt;- select(apadb,\n  DateTimeStamp = datetimestamp,\n  DO_obs = do_mgl,\n  Temp = temp,\n  Sal = sal,\n  PAR = totpar,\n  WSpd = wspd,\n  Depth = depth\n)\n\n# show first six rows\nhead(apadb)\n\n        DateTimeStamp DO_obs Temp  Sal PAR WSpd Depth\n1 2017-01-01 00:00:00    9.0 15.9 22.1   0  4.8  1.88\n2 2017-01-01 01:00:00    9.0 15.9 22.1   0  5.3  1.90\n3 2017-01-01 02:00:00    9.0 15.9 22.2   0  3.9  1.94\n4 2017-01-01 03:00:00    8.6 16.0 23.4   0  4.6  2.00\n5 2017-01-01 04:00:00    8.5 16.0 23.5   0  5.0  2.01\n6 2017-01-01 05:00:00    8.4 16.1 23.6   0  4.2  2.01\n\n# view structure\nstr(apadb)\n\nClasses 'swmpr' and 'data.frame':   26281 obs. of  7 variables:\n $ DateTimeStamp: POSIXct, format: \"2017-01-01 00:00:00\" \"2017-01-01 01:00:00\" ...\n $ DO_obs       : num  9 9 9 8.6 8.5 8.4 8.1 8.1 8.2 7.9 ...\n $ Temp         : num  15.9 15.9 15.9 16 16 16.1 16.2 16.2 16.1 16.3 ...\n $ Sal          : num  22.1 22.1 22.2 23.4 23.5 23.6 23.7 23.2 22.4 23.3 ...\n $ PAR          : num  0 0 0 0 0 ...\n $ WSpd         : num  4.8 5.3 3.9 4.6 5 4.2 4.3 5.3 4.9 5.6 ...\n $ Depth        : num  1.88 1.9 1.94 2 2.01 2.01 1.95 1.89 1.81 1.74 ...\n\n\nWe can verify the column names are the same between the two datasets.\n\nnames(apadb) %in% names(exdat)\n\n[1]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n\n\nThe last thing we need to do is to make sure the units are correct. Looking at the requirements in Section 2.3 (or the help file ?exdat) and those from the CDMO website, all of the data are in the correct units except for PAR. This should be in Watts per square meter, whereas the data are millimoles per square meter per 15 minute logging interval. Morel and Smith (1974) provide the following conversion factor:\n\\[\n1 \\, \\mu mol \\, m^{-2} \\, s^{-1} = 0.2175 \\, W \\, m^{-2}\n\\]\nBefore we apply the conversion factor, we need to convert the units of PAR from millimoles to micromoles and from per 15 minutes to per second. First we multiply PAR by 1000 to convert millimoles to micromoles and then divide by 900 to convert 15 minutes to seconds (15 minutes = 900 seconds). Lastly, we multiply the result by the constant above to convert micromoles per square meter per second to Watts per square meter. The final conversion will look like this:\n\\[\n1 \\, W \\, m^{-2} = 1 \\, mmol \\, m^{-2} \\, 15min^{-1} * 1000 / 900 * 0.2175\n\\]\nWe can use the mutate function in dplyr to convert PAR to the correct units in one step.\n\n# convert PAR to Watts per square meter\napadb &lt;- apadb |&gt; \n  mutate(\n    PAR = PAR * 1000 / 900 * 0.2175\n  )\n\nThe Apalachicola data should now work with EBASE. Let’s save the data for use in the Lesson 3.\n\nsave(apadb, file = 'data/apadb.RData')",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Preparation</span>"
    ]
  },
  {
    "objectID": "02_dataprep.html#data-assessment",
    "href": "02_dataprep.html#data-assessment",
    "title": "2  Data Preparation",
    "section": "2.7 Data assessment",
    "text": "2.7 Data assessment\nA few additional exploratory analyses are worthwhile before we use these data. Most functions in R deal with missing data by either not working at all or providing a work-around to accommodate the missing values. The functions in EBASE are in the latter category and we’ll discuss how this is done in Lesson 3. However, knowing how much and where missing observations occur is still important for interpreting results.\n\n\n\n\n\n\nNote\n\n\n\nAlways plot your data before using it in any analysis!\n\n\nMissing data in SWMP are not uncommon and they often occur when sondes or other equipment are down for maintenance or are otherwise not working for a period of time. Missing observations usually come in blocks where all parameters are unavailable, as opposed to only one parameter. Below, we can quickly see how many missing observations occur in column.\n\napply(apadb, 2, function(x) sum(is.na(x)))\n\nDateTimeStamp        DO_obs          Temp           Sal           PAR \n            0          6307          6307          6307          7816 \n         WSpd         Depth \n         7419          6307 \n\n\nThere are quite a few missing observations. Let’s create some quick plots to see where these occur. We’ll use the plotly R package that lets us dynamically interact with the data (Sievert 2020).\n\nlibrary(plotly)\n\nplot_ly(apadb, x = ~DateTimeStamp, y = ~DO_obs, type = 'scatter', mode = 'lines')\n\n\n\n\n\nThere’s a huge gap in the fall of 2018 and the latter half of 2019. Using these years to estimate metabolism may not be advisable.\nSimilarly, the weather data also have missing observations. What may have caused this gap?\n\nplot_ly(apadb, x = ~DateTimeStamp, y = ~WSpd, type = 'scatter', mode = 'lines')\n\n\n\n\n\nThis brief assessment can give us information on which years of data are useful to interpret for metabolism. A more detailed analysis of the quality of the data is needed before more formal analyses, including an assessment of the QC codes in the SWMP data. This quick assessment will suffice for now.\n\n\n\n\n\n\n Exercise 2\n\n\n\nRepeat the above examples but use the combined dataset for Apalachicola East Bay that you created in Exercise 1.\n\nCreate a new section in your script using Ctrl + Shift + R and give it an appropriate name.\nLoad the dplyr package with the library function.\nSimultaneously rename and select the columns for date/time (DateTimeStamp = datetimestamp), dissolved oxygen (DO_obs = do_mgl), water temperature (Temp = temp), salinity (Sal = sal), PAR (PAR = par), and wind speed (WSpd = wspd) with the select function from dplyr. Don’t forget to assign the new dataset to an object in your workspace (with &lt;-).\nConvert PAR to the correct units using mutate.\n\n\n\n\n\n\n\n Answers\n\n\n\n\n\n\n# select and rename columns\napaeb &lt;- select(apaeb,\n  DateTimeStamp = datetimestamp,\n  DO_obs = do_mgl,\n  Temp = temp,\n  Sal = sal,\n  PAR = totpar,\n  WSpd = wspd\n)\n\n# convert PAR\napaeb &lt;- apaeb |&gt; \n  mutate(\n    PAR = PAR * 1000 / 900 * 0.2175\n  )",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Preparation</span>"
    ]
  },
  {
    "objectID": "02_dataprep.html#preparing-other-data",
    "href": "02_dataprep.html#preparing-other-data",
    "title": "2  Data Preparation",
    "section": "2.8 Preparing other data",
    "text": "2.8 Preparing other data\nContinuous monitoring data from other sources can be used with EBASE. As above, the data must include relevant water quality and weather data at an appropriate time step. This section will show you how to prepare generic data with more general tools in R.\n\n\n\n\n\n\nNote\n\n\n\nEBASE can work with any continuous monitoring data.\n\n\nData from Tampa Bay will be used in this example. We’ll pull water quality data from a continuous monitoring platform in the lower bay and weather data from a nearby NOAA monitoring station.\nThe water quality data are available here and can be downloaded through an API. Take a look at the URL to see how the API is queried.\n\nurl &lt;- 'http://tampabay.loboviz.com/cgi-data/nph-data.cgi?node=82&min_date=20210701&max_date=20210801&y=salinity,temperature,oxygen,par&data_format=text'\n\nlobo &lt;- read.table(url, skip = 2, sep = '\\t', header = T)\nhead(lobo)\n\n           date..EST. salinity..PSU. temperature..C. dissolved.oxygen..mg.L.\n1 2021-07-01 00:30:00          23.95           28.59                    4.73\n2 2021-07-01 01:30:00          24.03           28.58                    4.56\n3 2021-07-01 02:30:00          25.26           28.84                    4.60\n4 2021-07-01 03:30:00          25.77           28.79                    4.41\n5 2021-07-01 04:30:00          26.12           28.86                    4.31\n6 2021-07-01 05:30:00          26.35           28.89                    4.36\n  PAR..uM.m.2.sec.\n1            0.044\n2            0.044\n3            0.044\n4            0.044\n5            0.044\n6            0.244\n\nstr(lobo)\n\n'data.frame':   768 obs. of  5 variables:\n $ date..EST.             : chr  \"2021-07-01 00:30:00\" \"2021-07-01 01:30:00\" \"2021-07-01 02:30:00\" \"2021-07-01 03:30:00\" ...\n $ salinity..PSU.         : num  23.9 24 25.3 25.8 26.1 ...\n $ temperature..C.        : num  28.6 28.6 28.8 28.8 28.9 ...\n $ dissolved.oxygen..mg.L.: num  4.73 4.56 4.6 4.41 4.31 4.36 4.48 4.45 4.49 4.76 ...\n $ PAR..uM.m.2.sec.       : num  0.044 0.044 0.044 0.044 0.044 ...\n\n\nThe data are not in the right format for EBASE. We need to change the names, convert the date/time column to a POSIXct object with the correct time zone, and convert the PAR data to the correct units. PAR is micromoles per square meter per second, so we need to convert this to Watts per square meter. First, we select and rename the columns with select.\n\nlobo &lt;- lobo |&gt; \n  select(\n    DateTimeStamp = date..EST.,\n    DO_obs = dissolved.oxygen..mg.L.,\n    Temp = temperature..C.,\n    Sal = salinity..PSU.,\n    PAR = PAR..uM.m.2.sec.\n  )\n\nNext, we use mutate to convert the date/time column to a POSIXct object and the PAR data to the correct units. For date/time, we can use the ymd_hms function from the lubridate package with the “America/Jamaica” time zone, which is eastern time without daylight savings. Note that this step is not needed for SWMP data since the SWMPr package automatically converts the date/time column using the correct time zone. The PAR column is converted using the same formula from above, except we don’t need to convert from millimoles to micromoles and from 15 minutes to seconds.\n\nlobo &lt;- lobo |&gt; \n  mutate(\n    DateTimeStamp = lubridate::ymd_hms(DateTimeStamp, tz = 'America/Jamaica'),\n    PAR = PAR * 0.2175\n  )\n\nNext we import the weather data. These can be similarly downloaded using the NOAA Tides & Currents API. Again, note how the URL is specified to get the data we want.\n\nurl &lt;- 'https://api.tidesandcurrents.noaa.gov/api/prod/datagetter?product=wind&application=NOS.COOPS.TAC.MET&begin_date=20210701&end_date=20210801&station=8726520&time_zone=LST&units=metric&format=CSV'\n\nports &lt;- read.table(url, sep = ',', header = T)\nhead(ports)\n\n         Date.Time Speed Direction Direction.1 Gust X R\n1 2021-07-01 00:00   1.2        88           E  2.0 0 0\n2 2021-07-01 00:06   1.6       101           E  2.2 0 0\n3 2021-07-01 00:12   0.8       101           E  2.3 0 0\n4 2021-07-01 00:18   1.5        83           E  2.3 0 0\n5 2021-07-01 00:24   1.4        89           E  2.2 0 0\n6 2021-07-01 00:30   1.3        94           E  2.5 0 0\n\nstr(ports)\n\n'data.frame':   7680 obs. of  7 variables:\n $ Date.Time  : chr  \"2021-07-01 00:00\" \"2021-07-01 00:06\" \"2021-07-01 00:12\" \"2021-07-01 00:18\" ...\n $ Speed      : num  1.2 1.6 0.8 1.5 1.4 1.3 1.2 1.2 1.8 2.4 ...\n $ Direction  : num  88 101 101 83 89 94 83 86 87 90 ...\n $ Direction.1: chr  \"E\" \"E\" \"E\" \"E\" ...\n $ Gust       : num  2 2.2 2.3 2.3 2.2 2.5 2.5 2.9 2.8 3.3 ...\n $ X          : int  0 0 0 0 0 0 0 0 0 0 ...\n $ R          : int  0 0 0 0 0 0 0 0 0 0 ...\n\n\nSimilar to the water quality data, we need to select and rename the required columns and convert the date/time column to a POSIXct object with the correct time zone. Instead of ymd_hms as above, we’ll use ymd_hm because seconds are not included. We’ll do this in one step.\n\nports &lt;- ports |&gt; \n  select(\n    DateTimeStamp = Date.Time,\n    WSpd = Speed\n  ) |&gt; \n  mutate(\n    DateTimeStamp = lubridate::ymd_hm(DateTimeStamp, tz = 'America/Jamaica')\n  )\n\nThe last step is to combine the water quality and weather data. We can use a simple join function from the dplyr package. The water quality and weather data are at different time steps, where the water quality data is every hour on the 30 minute mark and the weather data are every six minutes. Because the weather data also include observations on the 30 minute mark, we can just left join them to the water quality data, retaining only those data on the 30 minute mark. See here for more information about joins.\n\ntbdat &lt;- left_join(lobo, ports, by = 'DateTimeStamp')\n\nhead(tbdat)\n\n        DateTimeStamp DO_obs  Temp   Sal     PAR WSpd\n1 2021-07-01 00:30:00   4.73 28.59 23.95 0.00957  1.3\n2 2021-07-01 01:30:00   4.56 28.58 24.03 0.00957  2.2\n3 2021-07-01 02:30:00   4.60 28.84 25.26 0.00957  1.3\n4 2021-07-01 03:30:00   4.41 28.79 25.77 0.00957  2.0\n5 2021-07-01 04:30:00   4.31 28.86 26.12 0.00957  2.2\n6 2021-07-01 05:30:00   4.36 28.89 26.35 0.05307  3.0\n\nstr(tbdat)\n\n'data.frame':   768 obs. of  6 variables:\n $ DateTimeStamp: POSIXct, format: \"2021-07-01 00:30:00\" \"2021-07-01 01:30:00\" ...\n $ DO_obs       : num  4.73 4.56 4.6 4.41 4.31 4.36 4.48 4.45 4.49 4.76 ...\n $ Temp         : num  28.6 28.6 28.8 28.8 28.9 ...\n $ Sal          : num  23.9 24 25.3 25.8 26.1 ...\n $ PAR          : num  0.00957 0.00957 0.00957 0.00957 0.00957 ...\n $ WSpd         : num  1.3 2.2 1.3 2 2.2 3 2.9 1.7 2 1.3 ...\n\n\nWe see that the final dataset has the same number of rows as the water quality data and a wind speed column has been added from the weather data that includes data on the 30 minute mark. Now the data are ready for analysis.\n\n\n\n\n\n\n Exercise 3\n\n\n\nWe’ll want to visually evaluate the Tampa Bay data before using EBASE.\n\nCreate and name a section header in your script with Ctrl + Shift + R. Enter all exercise code in this section.\nLoad the plotly package with the library function.\nUse the plotly package to create some time series plots for variables of interest.\n\nLook for gaps or outliers.\nLook for interesting trends that might influence how you interpret metabolism.\n\n\n\n\n\n\n\n Answers\n\n\n\n\n\n\nlibrary(plotly)\n\nplot_ly(tbdat, x = ~DateTimeStamp, y = ~DO_obs, type = 'scatter', mode = 'lines')\nplot_ly(tbdat, x = ~DateTimeStamp, y = ~Temp, type = 'scatter', mode = 'lines')\nplot_ly(tbdat, x = ~DateTimeStamp, y = ~Sal, type = 'scatter', mode = 'lines')\nplot_ly(tbdat, x = ~DateTimeStamp, y = ~PAR, type = 'scatter', mode = 'lines')\nplot_ly(tbdat, x = ~DateTimeStamp, y = ~WSpd, type = 'scatter', mode = 'lines')",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Preparation</span>"
    ]
  },
  {
    "objectID": "02_dataprep.html#next-steps",
    "href": "02_dataprep.html#next-steps",
    "title": "2  Data Preparation",
    "section": "2.9 Next steps",
    "text": "2.9 Next steps\nThe most difficult part of using EBASE is preparing the data. Now that we have learned how to import and clean the data, we are ready for analysis. In the next lesson, we’ll learn about the theory behind EBASE, how to use it to estimate metabolism, and how to interpret the results.\n\n\n\n\nBeck, M. W. 2016. SWMPr: An R package for retrieving, organizing, and analyzing environmental data for estuaries. R Journal 8: 219–232.\n\n\nMorel, A., and R. C. Smith. 1974. Relation between total quanta and total energy for aquatic photosynthesis. Limnology and Oceanography 19: 591–600. doi:10.4319/lo.1974.19.4.0591\n\n\nSievert, C. 2020. Interactive web-based data visualization with r, plotly, and shiny, Chapman; Hall/CRC.",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Preparation</span>"
    ]
  },
  {
    "objectID": "03_ebase.html",
    "href": "03_ebase.html",
    "title": "3  Using EBASE",
    "section": "",
    "text": "3.1 Lesson Outline\nThis lesson will explain the basic theory behind ecosystem metabolism and how EBASE is used to estimate key parameters. EBASE is a Bayesian implementation for estimating metabolism that differs from more conventional approaches. We’ll also explore the main functions of EBASE to estimate metabolism, how the Bayesian approach can be used to incorporate prior knowledge, and interpret the results.",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Using EBASE</span>"
    ]
  },
  {
    "objectID": "03_ebase.html#learning-goals",
    "href": "03_ebase.html#learning-goals",
    "title": "3  Using EBASE",
    "section": "3.2 Learning Goals",
    "text": "3.2 Learning Goals\n\nUnderstand the basic theories of EBASE\nUnderstand the Bayesian approach to estimating metabolism\nLearn how EBASE is used to estimate key parameters\nUse the main functions of EBASE to estimate metabolism\nInterpret the results of EBASE",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Using EBASE</span>"
    ]
  },
  {
    "objectID": "03_ebase.html#the-ebase-method",
    "href": "03_ebase.html#the-ebase-method",
    "title": "3  Using EBASE",
    "section": "3.3 The EBASE method",
    "text": "3.3 The EBASE method\nLesson 1 provided a general description of ecosystem metabolism and why it’s an important measure of ecosystem health. The EBASE approach follows these general principles but differs in the core model for estimating production, respiration, and gas exchange and the statistical approach for how the parameters for each are estimated. Let’s revisit the core metabolic equation:\n\\[\nNEM = P - R\n\\]\nNet ecosystem metabolism (\\(NEM\\)) is simply the difference between processes that produce organic matter (production or \\(P\\)) and those that consume organic matter (respiration or \\(R\\)).\nWe can estimate these rates from the dissolved oxygen (\\(DO\\)) time series by using the rate of change of \\(DO\\) per unit time and accounting for gas exchange \\(D\\). Water column depth (\\(Z\\)) is used to estimate metabolism as an areal rate.\n\\[\nZ\\frac{dC_d}{dt} = P - R + D\n\\]\nMethods for estimating metabolism from the dissolved oxygen time series vary in how each of the key parameters are modeled. The core equation for EBASE is the same as the previous but expanded using methods for estuarine applications:\n\\[\nZ\\frac{dC_d}{dt} = aPAR - R + bU_{10}^2\\left(\\frac{Sc}{600} \\right)^{-0.5} \\left(C_{Sat} - C_d \\right )\n\\]\n\\(aPAR\\) is production (\\(P\\)), \\(R\\) is respiration, and the last term is gas exchange (\\(D\\), Wanninkhof 2014). The required input data to fit this model was described in Lesson 2. Three of these inputs are used directly in the equation as \\(C_d\\) for dissolved oxygen, \\(PAR\\), and wind speed as \\(U_{10}^2\\) (or squared wind speed at ten meters above the surface). The other terms are the Schmidt number (\\(S_c\\)) and dissolved oxygen at saturation (\\(C_{sat}\\)), both of which are calculated from water temperature and salinity in the input data.\n\n\n\n\n\n\nNote\n\n\n\nEBASE estimates \\(a\\), \\(R\\), and \\(b\\).\n\n\nThe remaining terms \\(a\\), \\(R\\), and \\(b\\) are estimated during the model fitting process, which we’ll discuss in Section 3.4. All three provide important information about metabolic processes and are useful on their own to understand ecosystem function.\n\n\\(a\\): The light efficiency parameter as \\(\\left(mmol~O_2~m^{-2}~d^{-1}\\right) / \\left(W~m^{-2}\\right)\\), yields production when multiplied by \\(PAR\\). This is a measure of how efficiently light is converted to organic matter. It provides similar information as a P/I curve by showing how production changes with light availability.\n\\(R\\): The respiration rate as \\(mmol~O_2~m^{-2}~d^{-1}\\), is the rate at which organic matter is consumed.\n\\(b\\): The sensitivity of gas exchange to wind speed as \\(\\left(cm~h^{-1}\\right) / \\left(m^2~s^{-2}\\right)\\). This can provide information on the importance of gas exchange in the system.\n\nAlthough the core model equation may seem complicated, its formulation was chosen to describe dominant processes that influence metabolism in an estuarine setting. The details for this justification can be found in Beck et al. (2024).",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Using EBASE</span>"
    ]
  },
  {
    "objectID": "03_ebase.html#sec-bayes",
    "href": "03_ebase.html#sec-bayes",
    "title": "3  Using EBASE",
    "section": "3.4 The Bayesian approach",
    "text": "3.4 The Bayesian approach\nBoth the open-water method presented by Odum (1956) and EBASE similarly use the rate of change of \\(DO\\) to estimate metabolism. However, the Bayesian approach used in EBASE differs in how the parameters are estimated and has several key advantages.\nModern statistical approaches can broadly be described as conventional “frequentist” methods or Bayesian. The former describes more commonly used methods where a model is fit to a dataset to support a hypothesis, as in a simple linear regression. Bayesian approaches turn the paradigm around by asking the question “what is the likelihood of the data given a model?”.\nBayesian approaches are fundamentally linked to Bayes’ theorem that describes conditional probabilities or the likelihood of observing an event given that other events have occurred. In model speak, this means we can estimate the posterior distribution of a parameter given a prior distribution.\n\n\n\n\n\n\nNote\n\n\n\nBayesian models require a prior distribution for each parameter.\n\n\nFor EBASE, we can establish a set of prior distributions of likely values for each of the unknown parameters, \\(a\\), \\(R\\), and \\(b\\). These prior distributions are used to create a refined estimate for the posterior distributions given the data that is presented to the model. The prior distributions can be precise (informed) or cover a range of potential values (uninformed).\nThe default prior distributions for EBASE were based on a likely range of values for each parameter that were informed by published results (primarily Caffrey 2004; details in Beck et al. 2024). You can create a plot of these prior distributions using the prior_plot function in EBASE.\n\nlibrary(EBASE)\n\nprior_plot()\n\n\n\n\n\n\n\n\nRunning EBASE with the default arguments will use these prior distributions to estimate the parameters. We’ll discuss how to change these values in Section 3.10 if you have additional prior information.\n\n\n\n\n\n\nNote\n\n\n\nPrior distributions can be informed or uninformed.\n\n\nFinally, the Bayesian approach in EBASE uses an MCMC (Markov Chain Monte Carlo) method to estimate the posterior distributions. This is implemented using the JAGS software which was part of the installation during setup for the workshop. These methods are used to estimate the posterior distributions by repeated sampling of the priors given the data until a stable estimate is reached (i.e., convergence). Because of this, estimation can take a while and we’ll talk about arguments in EBASE to control the process. See here for a more in depth introduction to JAGS.",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Using EBASE</span>"
    ]
  },
  {
    "objectID": "03_ebase.html#using-ebase",
    "href": "03_ebase.html#using-ebase",
    "title": "3  Using EBASE",
    "section": "3.5 Using EBASE",
    "text": "3.5 Using EBASE\nNext we’ll use EBASE to estimate metabolism using the dataset from Lesson 2. First, the dataset is loaded into your workspace.\n\nload(file = 'data/apadb.RData')\n\nWe’ll subset a few days so we can run the example in real time. This requires some functions from the dplyr package. We’ll use one week of data in March.\n\nlibrary(dplyr)\n\n# dates to subset\ndts &lt;- as.POSIXct(c(\"2017-03-07 00:00:00\", \"2017-03-14 00:00:00\"), tz = 'America/Jamaica')\n\n# subset apadb\napadbsub &lt;- apadb |&gt; \n  filter(DateTimeStamp &gt;= dts[1] & DateTimeStamp &lt;= dts[2])\n\nhead(apadbsub)\n\n        DateTimeStamp DO_obs Temp  Sal PAR WSpd Depth\n1 2017-03-07 00:00:00    8.6 18.6 11.0   0  3.4  1.78\n2 2017-03-07 01:00:00    8.9 18.7 10.3   0  6.1  1.73\n3 2017-03-07 02:00:00    8.7 18.6 11.8   0  6.2  1.64\n4 2017-03-07 03:00:00    8.8 18.5 10.1   0  6.4  1.56\n5 2017-03-07 04:00:00    8.4 18.4  9.3   0  5.8  1.47\n6 2017-03-07 05:00:00    8.7 18.5  9.1   0  5.1  1.41\n\n\nNow we run EBASE using the data subset and the default arguments. We must supply values for the interval argument that defines the time step of the data in seconds and the water column depth Z in meters. Because we combined the data at a 60 minute time step, we can use interval = 3600. The water column depth can be estimated from the data. We’ll also add a small amount to it because the pressure sensor is typically offset from the bottom.\n\n\n\n\n\n\nNote\n\n\n\nThe ebase() function only requires input data, time step, and water column depth.\n\n\n\n# get depth\nzval &lt;- mean(apadbsub$Depth, na.rm = T) + 0.3\n\n# run ebase\nres &lt;- ebase(apadbsub, interval = 3600, Z = zval)\n\nWarning: Incomplete daily observations removed at end of dat\n\n\nWarning: More than one time step or missing values will be interpolated\n\n\nWarning: executing %dopar% sequentially: no parallel backend registered\n\n\nLoading required package: rjags\n\n\nLoading required package: coda\n\n\nLinked to JAGS 4.3.0\n\n\nLoaded modules: basemod,bugs\n\n\n\nAttaching package: 'R2jags'\n\n\nThe following object is masked from 'package:coda':\n\n    traceplot\n\n\nFirst off, we got a few warnings in the console telling us some information about our dataset. Both of these warnings indicate the default behavior for EBASE when incomplete daily observations at the start or end of the dataset are present and if missing observations are found. As the warnings indicate, the “dangling” observations are removed since it is impossible to estimate metabolism accurately for these data. Missing data in the middle of the time series are also interpolated and these can be handled differently depending on the user’s preference. We’ll describe more about this in Section 3.9.\nLet’s look at the output.\n\nhead(res)\n\n        DateTimeStamp       Date grp        Z  DO_obs   DO_mod DO_modlo\n1 2017-03-07 00:00:00 2017-03-07   1 1.717083 268.750 270.8333 270.8314\n2 2017-03-07 01:00:00 2017-03-07   1 1.717083 278.125 269.3205 268.0669\n3 2017-03-07 02:00:00 2017-03-07   1 1.717083 271.875 268.0298 265.6517\n4 2017-03-07 03:00:00 2017-03-07   1 1.717083 275.000 266.7119 263.2662\n5 2017-03-07 04:00:00 2017-03-07   1 1.717083 262.500 265.6931 261.4019\n6 2017-03-07 05:00:00 2017-03-07   1 1.717083 271.875 264.7194 259.3586\n  DO_modhi       dDO converge       rsq         a       alo      ahi         b\n1 270.8353        NA     Fine 0.1840882        NA        NA       NA        NA\n2 270.4468 -36.30767     Fine 0.1840882 0.6233181 0.2292881 1.098696 0.2486319\n3 270.2101 -30.97790     Fine 0.1840882 0.6233181 0.2292881 1.098696 0.2486319\n4 269.8746 -31.62892     Fine 0.1840882 0.6233181 0.2292881 1.098696 0.2486319\n5 269.8640 -24.45129     Fine 0.1840882 0.6233181 0.2292881 1.098696 0.2486319\n6 269.7869 -23.36881     Fine 0.1840882 0.6233181 0.2292881 1.098696 0.2486319\n         blo       bhi  P Plo Phi        R      Rlo      Rhi         D\n1         NA        NA NA  NA  NA       NA       NA       NA        NA\n2 0.04482773 0.4501714  0   0   0 64.22865 17.36652 117.0242  1.885348\n3 0.04482773 0.4501714  0   0   0 64.22865 17.36652 117.0242 11.037011\n4 0.04482773 0.4501714  0   0   0 64.22865 17.36652 117.0242  9.919155\n5 0.04482773 0.4501714  0   0   0 64.22865 17.36652 117.0242 22.243748\n6 0.04482773 0.4501714  0   0   0 64.22865 17.36652 117.0242 24.102462\n        Dlo       Dhi\n1        NA        NA\n2 0.3398168  3.413524\n3 1.8475525 21.937421\n4 1.4699728 22.284321\n5 3.6605093 45.659507\n6 4.1069630 48.430889\n\n\nThe results are returned as a data frame with instantaneous metabolic estimates for areal gross production (O2 mmol m\\(^{-2}\\) d\\(^{-1}\\), P), respiration (O2 mmol m\\(^{-2}\\) d\\(^{-1}\\), R), and gas exchange (O2 mmol m\\(^{-2}\\) d\\(^{-1}\\), D or the remainder of the equation from above, positive values as ingassing, negative values as outgassing).\n\n\n\n\n\n\nNote\n\n\n\nebase() returns modeled dissolved oxygen and metabolic parameters with their credible intervals.\n\n\nAdditional parameters estimated by the model that are returned include a as (mmol O\\(_2\\) m\\(^{-2}\\) d\\(^{-1}\\))/(W m\\(^{-2}\\)) and b as (cm h\\(^{-1}\\))/(m\\(^2\\) s\\(^{-2}\\)). We also see modelled DO as DO_mod and various convergence metrics. The upper and lower credible intervals for each estimated parameter are also provided by lo or hi suffixes, which we’ll describe later to assess model fit.\n\n\n\n\n\n\n Exercise 1\n\n\n\nRepeat the above example but use a different date range.\n\nCreate and name a section header in your script with Ctrl + Shift + R. Enter all exercise code in this section.\nCreate another date range of similar length (seven days).\nSubset the apadb data to this new date range.\nRun ebase() on the new data subset. Just use 1.7 for the depth and don’t forget to enter a value for interval.\nExamine the results - what do the columns tell you?\n\n\n\n\n\n\n\n Answers\n\n\n\n\n\n\n# dates to subset\ndts &lt;- as.POSIXct(c(\"2012-07-01 00:00:00\", \"2012-07-07 00:00:00\"), tz = 'America/Jamaica')\n\n# subset apadb\napadbsub &lt;- apadb |&gt; \n  filter(DateTimeStamp &gt;= dts[1] & DateTimeStamp &lt;= dts[2])\n\n# run ebase\nres &lt;- ebase(apadbsub, interval = 3600, Z = 1.7)\n\nhead(res)\n\n\n\n\n\n\nWe’ll use some additional functions in EBASE to interpret the results. A plot of the results can be made with ebase_plot(). This plot shows \\(P\\), \\(R\\), and \\(D\\) as instantaneous values at the same time step as the original dataset. Note that a single value for \\(R\\) is returned each day. The model assumes respiration is constant for each period of time the model is estimated. This is called the “optimization period” and it is set as 1 day by default. The grp column in the model output indicates which optimization period applies to each observation. We’ll talk more about how and why you might change this value in Section 3.8.\n\nebase_plot(res)\n\n\n\n\n\n\n\n\nThe daily averages can also be plotted by using instantaneous = FALSE.\n\nebase_plot(res, instantaneous = FALSE)\n\n\n\n\n\n\n\n\nNote that \\(NEM\\) is not returned by ebase(), nor is it included in the plots. We can easily calculate \\(NEM\\) by subtracting \\(R\\) from \\(P\\) and use the ggplot2 package to plot the result.\n\n# calculate NEM\nres &lt;- res |&gt; \n  mutate(\n    NEM = P - R\n  )\n\n# make a plot\nlibrary(ggplot2)\n\nggplot(res, aes(x = DateTimeStamp, y = NEM)) +\n  geom_line() +\n  geom_hline(yintercept = 0, linetype = 'dashed') +\n  theme_minimal() + \n  labs(\n    x = NULL,\n    y = 'NEM (mmol O2 m-2 d-1)'\n  )\n\n\n\n\n\n\n\n\nOr we can average NEM each day and plot it again.\n\n# calculate daily NEM\nnemdly &lt;- res |&gt; \n  summarise(\n    NEM = mean(NEM, na.rm = T), \n    .by = Date\n  )\n\n# plot daily NEM\nggplot(nemdly, aes(x = Date, y = NEM)) +\n  geom_line() +\n  geom_hline(yintercept = 0, linetype = 'dashed') +\n  theme_minimal() +\n  labs(\n    x = NULL,\n    y = 'NEM (mmol O2 m-2 d-1)'\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNEM can be estimated by hand using the output from ebase().",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Using EBASE</span>"
    ]
  },
  {
    "objectID": "03_ebase.html#assess-model-fit",
    "href": "03_ebase.html#assess-model-fit",
    "title": "3  Using EBASE",
    "section": "3.6 Assess model fit",
    "text": "3.6 Assess model fit\nModel fit can be assessed using the converge and rsq columns from the returned results. The values in these columns apply to each group in the grp column as specified with the ndays argument. The converge column indicates \"Check convergence\" or \"Fine\" if the JAGS estimate converged at that iteration or optimization period (repeated across rows for the group). Similarly, the rsq column shows the r-squared values of the linear fit between the modeled and observed dissolved oxygen (repeated across rows for the group).\nThe model fit can also be assessed by comparing the observed and modeled values for dissolved oxygen with the fit_plot() function. Estimated values are shown as lines and observed values are shown as points.\n\nfit_plot(res)\n\n\n\n\n\n\n\n\nThe comparison can also be separated by group with bygroup = TRUE based on the value for the ndays argument passed to ebase(), default as 1 day. The r-squared value of the fit between modeled and observed dissolved oxygen is also shown in the facet label for the group.\n\nfit_plot(res, bygroup = TRUE)\n\n\n\n\n\n\n\n\nA scatterplot showing modeled versus observed dissolved oxygen can also be returned by setting scatter = TRUE.\n\nfit_plot(res, bygroup = TRUE, scatter = TRUE)\n\n\n\n\n\n\n\n\n95% credible intervals for a, R, and b are also returned with the output from ebase() in the corresponding columns alo, ahi, blo, bhi, Rlo, and Rhi, for the 2.5th and 97.5th percentile estimates for each parameter, respectively. These values indicate the interval within which there is a 95% probability that the true parameter is in this range and is a representation of the posterior distributions for each parameter.\n\n\n\n\n\n\nNote\n\n\n\nAssess model fit using the converge and rsq columns in the output, plotting the modeled DO with fit_plot(), and checking the credible intervals with credible_plot().\n\n\nThe credible intervals can be plotted with the credible_plot() function.\n\ncredible_plot(res)\n\n\n\n\n\n\n\n\nThe credible intervals can also be retrieved as a data frame using credible_prep(). This function is provided as a convenience to parse the results from ebase().\n\ncredible_prep(res)\n\n# A tibble: 21 × 6\n# Groups:   grp [7]\n   Date         grp var      mean      lo      hi\n   &lt;date&gt;     &lt;dbl&gt; &lt;fct&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 2017-03-07     1 a       0.623  0.229    1.10 \n 2 2017-03-07     1 R      64.2   17.4    117.   \n 3 2017-03-07     1 b       0.249  0.0448   0.450\n 4 2017-03-08     2 a       0.869  0.321    1.59 \n 5 2017-03-08     2 R      30.6    2.17    71.6  \n 6 2017-03-08     2 b       0.259  0.0368   0.471\n 7 2017-03-09     3 a       1.78   1.23     2.38 \n 8 2017-03-09     3 R     133.    76.6    194.   \n 9 2017-03-09     3 b       0.328  0.117    0.488\n10 2017-03-10     4 a       0.348  0.0257   0.883\n# ℹ 11 more rows",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Using EBASE</span>"
    ]
  },
  {
    "objectID": "03_ebase.html#using-ebase-with-long-time-series",
    "href": "03_ebase.html#using-ebase-with-long-time-series",
    "title": "3  Using EBASE",
    "section": "3.7 Using EBASE with long time series",
    "text": "3.7 Using EBASE with long time series\nExecution time of the model can also be reduced by using multiple processors. This is done using the doParrallel package and creating a parallel backend as below. It doesn’t really help us when the time series is short, but it can be useful when running more than a few days of data.\n\n# setup parallel backend\nlibrary(doParallel)\nncores &lt;- detectCores() - 2\ncl &lt;- makeCluster(ncores)\nregisterDoParallel(cl)\n\nres &lt;- ebase(apadbsub, interval = 3600, Z = zval)\n\nstopCluster(cl)\n\n\n\n\n\n\n\nNote\n\n\n\nebase() can be run on long time series with parallel processing.\n\n\nFinally, although ebase() can be used to estimate metabolism for time series with several years of data, the ebase_years() function can be used to estimate results sequentially for each year. This is useful because model estimation using ebase_years() will continue after a year fails, e.g., when some years have long periods of missing or erroneous data. This eliminates the need to restart the model or further pre-process the data. The same arguments for ebase() are used for ebase_years(). Progress is printed directly in the console and the user can specify the number of attempts for failed years before proceeding to the following year.\n\nzval &lt;- mean(apadb$Depth, na.rm = T)\napadbfull &lt;- ebase_years(apadb, Z = zval, interval = 3600, ncores = ncores, quiet = F)\nsave(apadbfull, file = 'data/apadbfull.RData')",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Using EBASE</span>"
    ]
  },
  {
    "objectID": "03_ebase.html#sec-optimization",
    "href": "03_ebase.html#sec-optimization",
    "title": "3  Using EBASE",
    "section": "3.8 Equation optimization length",
    "text": "3.8 Equation optimization length\nThe ndays argument in ebase() defines the model optimization period as the number of days that are used for fitting the model. By default, this is done each day, i.e., ndays = 1. Individual parameter estimates for a, R, and b are then returned for each day. However, more days can be used to estimate the unknown parameters - one day may not provide enough data to the model to create a reliable estimate.\n\n\n\n\n\n\nNote\n\n\n\nThe model optimization period depends on your analysis goals.\n\n\nHere, the number of days used to optimize the equation is set to all days in the input data.\n\nres &lt;- ebase(apadbsub, interval = 3600, Z = zval, ndays = 7)\n\nAnd the resulting plot:\n\nebase_plot(res)\n\n\n\n\n\n\n\n\nAnd the fit of observed and modeled dissolved oxygen (note the unbroken line for all days estimated together):\n\nfit_plot(res)\n\n\n\n\n\n\n\n\nChoosing the number of days to optimize the equation is a user choice based on questions of interest for the dataset. Typically, using one day for optimization may not provide enough data to meaningfully describe metabolism, although this may be the only option for very short time series. Longer optimization periods can be used for longer time series, but key events driving metabolism may be missed if the optimization period is too long, e.g., monthly.\n\n\n\n\n\n\n Exercise 2\n\n\n\nRun ebase() on a different data subset, change the optimization period, and examine the results.\n\nCreate and name a section header in your script with Ctrl + Shift + R. Enter all exercise code in this section.\nCreate another date range of similar length for 2017.\nSubset the apadb data to this new date range.\nEstimate average depth from the data subset.\nRun ebase() on the new data subset using a different optimization length for the ndays argument. Make sure to also pass the Z argument with the average depth.\nExamine the results using fit_plot, ebase_plot, and credible_plot.\n\n\n\n\n\n\n\n Answers\n\n\n\n\n\n\n# dates to subset\ndts &lt;- as.POSIXct(c(\"2017-08-01 00:00:00\", \"2017-08-08 00:00:00\"), tz = 'America/Jamaica')\n\n# subset exdat\napadbsub &lt;- apadb |&gt; \n  filter(DateTimeStamp &gt;= dts[1] & DateTimeStamp &lt;= dts[2])\n\n# get average depth\nzval &lt;- mean(apadbsub$Depth, na.rm = T)\n\n# run ebase\nres &lt;- ebase(apadbsub, interval = 3600, Z = zval, ndays = 2)\n\nfit_plot(res)\nebase_plot(res)\ncredible_plot(res)",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Using EBASE</span>"
    ]
  },
  {
    "objectID": "03_ebase.html#sec-missing",
    "href": "03_ebase.html#sec-missing",
    "title": "3  Using EBASE",
    "section": "3.9 Missing values",
    "text": "3.9 Missing values\nMissing values in the input data are interpolated by ebase() prior to estimating metabolism. It is the responsibility of the user to verify that these interpolated values are not wildly inaccurate. Missing values are linearly interpolated between non-missing values at the time step specified by the value in interval. This works well for small gaps, but can easily create inaccurate values at gaps larger than a few hours.\n\n\n\n\n\n\nNote\n\n\n\nIt is not necessary to remove missing data before running ebase().\n\n\nLet’s create some missing data in our Apalachicola subset. We’ll remove a few hours on one day and remove an entire day.\n\napadbmiss &lt;- apadbsub\napadbmiss$DO_obs[c(57:61, 97:120)] &lt;- NA\n\nThe interpolated values can be visually inspected using the interp_plot() function.\n\ninterp_plot(apadbmiss, Z = zval, interval = 3600, param = 'DO_obs')\n\n\n\n\n\n\n\n\nThe ebase() function includes the maxinterp argument to assign NA values to continuously interpolated rows with length greater than the value defined by maxinterp. This value is set to 12 hours by default and applies to the groupings defined by ndays, i.e., any group with a continuous set of interpolated values where the time is greater than 12 hours are assigned NA (except Date and DateTimeStamp). The numeric value passed to maxinterp is the number of time steps for the input data.\nRunning ebase() on the time series with missing data shows how the interpolatd data are handled. Note that results for the long gap are removed, whereas those for the shorter gaps are retained.\n\nres &lt;- ebase(apadbmiss, interval = 3600, Z = zval)\nebase_plot(res)",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Using EBASE</span>"
    ]
  },
  {
    "objectID": "03_ebase.html#sec-changingpriors",
    "href": "03_ebase.html#sec-changingpriors",
    "title": "3  Using EBASE",
    "section": "3.10 Changing priors",
    "text": "3.10 Changing priors\nA main advantage of the Bayesian approach is the use of prior information to estimate parameters. By default, the prior distributions for the \\(a\\), \\(R\\), and \\(b\\) parameters are informed by the literature, although they are generally uninformed by allowing a large range of values to be considered given the data. We can further constrain the parameters if we have reason to do so.\n\n\n\n\n\n\nNote\n\n\n\nChange the prior distributions for ebase() using the aprior, rprior, and bprior arguments.\n\n\nHere, the prior distribution for the \\(b\\) parameter is fixed to 0.251 (cm h\\(^{-1}\\))/(m\\(^2\\) s\\(^{-2}\\)), as suggested by Wanninkhof (2014). We can view this change with the prior_plot function before using ebase(). Note that we define the mean as our desired value and assign a very small range for the standard deviation.\n\nprior_plot(bprior = c(0.251, 1e-6))\n\n\n\n\n\n\n\n\nThe same change to the prior distribution for the \\(b\\) parameter is applied to ebase()\n\nres &lt;- ebase(apadbsub, interval = 3600, Z = zval, bprior = c(0.251, 1e-6))\nebase_plot(res)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nUse a very small value for the standard deviation to fix the prior distribution to a constant.\n\n\nThe credible_plot function can be used to assess how changing the prior distributions has an influence on the posterior distributions of the parameters.\n\ncredible_plot(res)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Exercise 3\n\n\n\nRun ebase() using a different set of prior distributions.\n\nCreate and name a section header in your script with Ctrl + Shift + R. Enter all exercise code in this section.\nUse the same data subset and mean depth as the previous exercise.\nChange the prior distribution for the \\(a\\) parameter to a mean of 1 and a standard deviation of 1 and the prior distribution for the \\(b\\) parameter to a mean of 0.251 and a standard deviation of 1e-6.\nBefore running ebase(), evaluate the prior distribution using prior_plot.\nRun ebase() using the new prior distribution. Use ndays = 2.\nExamine the results using fit_plot, ebase_plot, and credible_plot.\n\n\n\n\n\n\n\n Answers\n\n\n\n\n\n\n# dates to subset\ndts &lt;- as.POSIXct(c(\"2017-08-01 00:00:00\", \"2017-08-08 00:00:00\"), tz = 'America/Jamaica')\n\n# subset exdat\napadbsub &lt;- apadb |&gt; \n  filter(DateTimeStamp &gt;= dts[1] & DateTimeStamp &lt;= dts[2])\n\n# get average depth\nzval &lt;- mean(apadbsub$Depth, na.rm = T)\n\n# use prior_plot\nprior_plot(aprior = c(1, 1), bprior = c(0.251, 1e-6))\n\n# run ebase\nres &lt;- ebase(apadbsub, interval = 3600, Z = zval, aprior = c(1, 1), bprior = c(0.251, 1e-6), ndays = 2)\n\nfit_plot(res)\nebase_plot(res)\ncredible_plot(res)",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Using EBASE</span>"
    ]
  },
  {
    "objectID": "03_ebase.html#next-steps",
    "href": "03_ebase.html#next-steps",
    "title": "3  Using EBASE",
    "section": "3.11 Next steps",
    "text": "3.11 Next steps\nYou now understand the basics of ecosystem metabolism with EBASE and how it estimates key parameters using a Bayesian approach. We’ve explored additional functions in EBASE to help interpret the results, which include an evaluation of goodness of fit and various plotting methods. Next, we’ll explore the EBASE results in more detail to demonstrate how the results can inform the understanding of ecosystem health and function.\n\n\n\n\nBeck, M. W., J. M. Arriola, M. Herrmann, and R. G. Najjar. 2024. Fitting metabolic models to dissolved oxygen data: The estuarine bayesian single-station estimation method. Limnology and Oceanography: Methods 22: 590–607. doi:10.1002/lom3.10620\n\n\nCaffrey, J. M. 2004. Factors controlling net ecosystem metabolism in U.S. estuaries. Estuaries 27: 90–101. doi:10.1007/bf02803563\n\n\nOdum, H. T. 1956. Primary production in flowing waters. Limnology and Oceanography 1: 102–117.\n\n\nWanninkhof, R. 2014. Relationship between wind speed and gas exchange over the ocean revisited. Limnology and Oceanography: Methods 12: 351–362. doi:10.4319/lom.2014.12.351",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Using EBASE</span>"
    ]
  },
  {
    "objectID": "04_interpret.html",
    "href": "04_interpret.html",
    "title": "4  Interpreting Results",
    "section": "",
    "text": "4.1 Discussion Outline\nThis discussion is your time to share some ideas about evaluating the results from EBASE. We’ll go through some live coding examples based on these ideas. Below are some examples and code to get us started.",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Interpreting Results</span>"
    ]
  },
  {
    "objectID": "04_interpret.html#example-1-piney-point",
    "href": "04_interpret.html#example-1-piney-point",
    "title": "4  Interpreting Results",
    "section": "4.2 Example 1: Piney Point",
    "text": "4.2 Example 1: Piney Point\nThis example evaluates the effects of a large influx of inorganic nutrients into Tampa Bay from a legacy fertilizer processing.\n\n\nCode\nlibrary(tidyverse)\nlibrary(EBASE)\nlibrary(doParallel)\nlibrary(here)\nlibrary(patchwork)\n\n# prep data -----------------------------------------------------------------------------------\n\nurl &lt;- 'http://tampabay.loboviz.com/cgi-data/nph-data.cgi?node=82&min_date=20210701&max_date=20210801&y=salinity,temperature,oxygen,par,pressure&data_format=text'\n\nlobo &lt;- read.table(url, skip = 2, sep = '\\t', header = T) |&gt; \n  select(\n    DateTimeStamp = date..EST.,\n    DO_obs = dissolved.oxygen..mg.L.,\n    Temp = temperature..C.,\n    Sal = salinity..PSU.,\n    PAR = PAR..uM.m.2.sec.,\n    Tide = pressure..dBar.\n  ) |&gt; \n  mutate(\n    DateTimeStamp = lubridate::ymd_hms(DateTimeStamp, tz = 'America/Jamaica'),\n    PAR = PAR * 0.2175, \n    Tide = Tide + 2.4\n  )\n\nurl &lt;- 'https://api.tidesandcurrents.noaa.gov/api/prod/datagetter?product=wind&application=NOS.COOPS.TAC.MET&begin_date=20210701&end_date=20210801&station=8726520&time_zone=LST&units=metric&format=CSV'\n\nports &lt;- read.table(url, sep = ',', header = T) |&gt; \n  select(\n    DateTimeStamp = Date.Time,\n    WSpd = Speed\n  ) |&gt; \n  mutate(\n    DateTimeStamp = lubridate::ymd_hm(DateTimeStamp, tz = 'America/Jamaica')\n  )\n\ntbdat &lt;- left_join(lobo, ports, by = 'DateTimeStamp')\n\n# exploratory plots ---------------------------------------------------------------------------\n\ntoplo &lt;- tbdat |&gt; \n  pivot_longer(-DateTimeStamp) \n\nggplot(tbdat, aes( x = DateTimeStamp)) +\n  geom_line(aes(y = value), data = toplo) +\n  facet_wrap(~name, scales = 'free_y', ncol = 1) +\n  theme_minimal()\n\n# run EBASE -----------------------------------------------------------------------------------\n\n# setup parallel backend\nncores &lt;- detectCores()\ncl &lt;- makeCluster(ncores - 2)\nregisterDoParallel(cl)\n\ntbdatebase &lt;- ebase(tbdat, interval = 3600, Z = tbdat$Tide, ndays = 1)\n\nstopCluster(cl)\n\nsave(tbdatebase, file = here('data/tbdatebase.RData'))\n\n# evaluate fit --------------------------------------------------------------------------------\n\nload(file = here('data/tbdatebase.RData'))\n\nfit_plot(tbdatebase)\nfit_plot(tbdatebase, bygroup = T)\nfit_plot(tbdatebase, bygroup = T, scatter = T)\n\ntable(tbdatebase$converge)\nunique(tbdatebase$rsq)\nrange(tbdatebase$rsq)\n\ncredible_plot(tbdatebase)\n\n# evaluate metabolism -------------------------------------------------------------------------\n\nebase_plot(tbdatebase)\nebase_plot(tbdatebase, instantaneous = F)\n\ntoplo &lt;- tbdatebase |&gt; \n  summarise(\n    P = mean(P, na.rm = T),\n    R = mean(R, na.rm = T), \n    .by = Date\n  ) |&gt; \n  mutate(\n    NEM = P - R, \n    R = -R\n  ) |&gt; \n  pivot_longer(cols = c(P, R, NEM), names_to = 'Variable', values_to = 'Value')\n\np1 &lt;- ggplot(toplo, aes(x = Date, y = Value, color = Variable)) +\n  geom_hline(yintercept = 0) +\n  geom_line() + \n  geom_point() + \n  theme_bw() + \n  labs(\n    x = NULL, \n    y = expression(paste('mmol ', O [2], ' ', m^-2, d^-1)), \n    color = 'Estimate'\n  )\n\np2 &lt;- ggplot(tbdatebase, aes(x = DateTimeStamp, y = DO_obs)) +\n  geom_line() + \n  theme_bw() + \n  labs(\n    x = NULL, \n    y = 'DO (mg/L)'\n  )\n\np1 + p2 + plot_layout(ncol = 1)",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Interpreting Results</span>"
    ]
  },
  {
    "objectID": "04_interpret.html#example-2-apalachicola-intersite-comparisons",
    "href": "04_interpret.html#example-2-apalachicola-intersite-comparisons",
    "title": "4  Interpreting Results",
    "section": "4.3 Example 2: Apalachicola intersite comparisons",
    "text": "4.3 Example 2: Apalachicola intersite comparisons\nThis example compares metabolism at two locations in Apalachicola Bay.\n\n\nCode\nlibrary(SWMPr)\nlibrary(plotly)\nlibrary(EBASE)\nlibrary(doParallel)\nlibrary(here)\nlibrary(tidyverse)\n\n# data prep -----------------------------------------------------------------------------------\n\n# weather\napaebmet &lt;- import_local(path = 'data/367272.zip', station_code = 'apaebmet')\napaebmet &lt;- qaqc(apaebmet, qaqc_keep = c('0', '1', '2', '3', '4', '5'))\n\n# apadb\napadbwq &lt;- import_local(path = 'data/367272.zip', station_code = 'apadbwq')\napadbwq &lt;- qaqc(apadbwq, qaqc_keep = c('0', '1', '2', '3', '4', '5'))\napadb &lt;- comb(apadbwq, apaebmet, timestep = 60, method = 'union')\napadb &lt;- select(apadb,\n                DateTimeStamp = datetimestamp,\n                DO_obs = do_mgl,\n                Temp = temp,\n                Sal = sal,\n                PAR = totpar,\n                WSpd = wspd,\n                Depth = depth\n) |&gt; \n  mutate(\n    PAR = PAR * 1000 / 900 * 0.2175\n  )\n\n# apaeb\napaebwq &lt;- import_local(path = 'data/367272.zip', station_code = 'apaebwq')\napaebwq &lt;- qaqc(apaebwq, qaqc_keep = c('0', '1', '2', '3', '4', '5'))\napaebmet &lt;- import_local(path = 'data/367272.zip', station_code = 'apaebmet')\napaebmet &lt;- qaqc(apaebmet, qaqc_keep = c('0', '1', '2', '3', '4', '5'))\napaeb &lt;- comb(apaebwq, apaebmet, timestep = 60, method = 'union')\napaeb &lt;- select(apaeb,\n                DateTimeStamp = datetimestamp,\n                DO_obs = do_mgl,\n                Temp = temp,\n                Sal = sal,\n                PAR = totpar,\n                WSpd = wspd,\n                Depth = depth\n) |&gt; \n  mutate(\n    PAR = PAR * 1000 / 900 * 0.2175\n  )\n\n# exploratory plots ---------------------------------------------------------------------------\n\np1 &lt;- plot_ly(apadb, x = ~DateTimeStamp, y = ~DO_obs, type = 'scatter', mode = 'lines')\np2 &lt;- plot_ly(apaeb, x = ~DateTimeStamp, y = ~DO_obs, type = 'scatter', mode = 'lines')\np3 &lt;- plot_ly(apaeb, x = ~DateTimeStamp, y = ~PAR, type = 'scatter', mode = 'lines')\n\nsubplot(p1, p2, p3, nrows = 3)\n\n# run EBASE -----------------------------------------------------------------------------------\n\n# this takes several hours!\n\nncores &lt;- detectCores() - 2\n\napadbzval &lt;- mean(apadb$Depth, na.rm = T)\napadbfull &lt;- ebase_years(apadb, Z = apadbzval, interval = 3600, ncores = ncores, quiet = F)\nsave(apadbfull, file = 'data/apadbfull.RData')\n\napaebzval &lt;- mean(apaeb$Depth, na.rm = T)\napaebfull &lt;- ebase_years(apaeb, Z = apaebzval, interval = 3600, ncores = ncores, quiet = F)\nsave(apaebfull, file = 'data/apaebfull.RData')\n\n# evaluate fit --------------------------------------------------------------------------------\n\nload(file = here('data/apadbfull.RData'))\nload(file = here('data/apaebfull.RData'))\n\ntable(apadbfull$converge)\nrange(apadbfull$rsq, na.rm = T)\n\nlowrsq &lt;- sort(unique(apadbfull$rsq), decreasing = F)[1:10]\nfit_plot(apadbfull[which(apadbfull$rsq %in% lowrsq), ], bygroup = T)\n\ncredible_plot(apadbfull)\n\ntable(apaebfull$converge)\nrange(apaebfull$rsq, na.rm = T)\n\nlowrsq &lt;- sort(unique(apaebfull$rsq), decreasing = F)[1:10]\nfit_plot(apaebfull[which(apaebfull$rsq %in% lowrsq), ], bygroup = T)\n\ncredible_plot(apaebfull)\n\n# evaluate metabolism -------------------------------------------------------------------------\n\nwxdat &lt;- apadb |&gt; \n  mutate(\n    Date = as.Date(DateTimeStamp)\n  ) |&gt; \n  summarise(\n    PAR = mean(PAR, na.rm = T),\n    Sal = mean(Sal, na.rm = T),\n    WSpd = mean(WSpd, na.rm = T), \n    Temp = mean(Temp, na.rm = T),\n    .by = Date\n  ) |&gt; \n  filter(year(Date) == 2017)\n\napacmb &lt;- list(\n    `Dry Bar` = apadbfull,\n    `East Bay` = apaebfull\n  ) |&gt; \n  enframe() |&gt; \n  unnest('value') |&gt; \n  filter(year(Date) == 2017) |&gt;\n  select(name, Date, P, R, a, b) |&gt; \n  mutate(NEM = P - R) |&gt; \n  summarise(\n    a = mean(a, na.rm = T),\n    b = mean(b, na.rm = T),\n    NEM = mean(NEM, na.rm = T),\n    R = mean(R, na.rm = T),\n    P = mean(P, na.rm = T),\n    .by = c(name, Date)\n  ) |&gt; \n  left_join(wxdat, by = 'Date')\n\nggplot(apacmb, aes(x = Date, y = NEM)) +\n  geom_hline(yintercept = 0) +\n  geom_line() + \n  geom_point() + \n  facet_wrap(~name, ncol = 1) +\n  theme_bw() + \n  labs(\n    x = NULL, \n    y = 'NEM'\n  )\n\nggplot(apacmb, aes(x = WSpd, y = b)) + \n  geom_point() + \n  geom_smooth(method = 'lm', se = F, formula = y~x) +\n  theme_bw() + \n  facet_wrap(~name)\n\nggplot(apacmb, aes(x = PAR, y = P)) + \n  geom_point() + \n  geom_smooth(method = 'lm', se = F, formula = y~x) +\n  theme_bw() + \n  facet_wrap(~name)\n\nggplot(apacmb, aes(x = Temp, y = R)) + \n  geom_point() + \n  geom_smooth(method = 'lm', se = F, formula = y~x) +\n  theme_bw() + \n  facet_wrap(~name)\n\nggplot(apacmb, aes(x = Sal, y = NEM)) +\n  geom_point() + \n  geom_smooth(method = 'lm', se = F, formula = y~x) +\n  theme_bw() + \n  facet_wrap(~name)",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Interpreting Results</span>"
    ]
  },
  {
    "objectID": "04_interpret.html#next-steps",
    "href": "04_interpret.html#next-steps",
    "title": "4  Interpreting Results",
    "section": "4.4 Next steps",
    "text": "4.4 Next steps\nThis concludes our workshop on using EBASE to estimate ecosystem metabolism. You should now have a baseline understanding of how these tools can be used to gain insights into ecosystem properties and the factors that may be influencing them. Please follow up with the instructor if you have additional questions or would like to explore more advanced topics.",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Interpreting Results</span>"
    ]
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Appendix A — Setup for the workshop",
    "section": "",
    "text": "A.1 Install R and RStudio\nR and RStudio are separate downloads and installations. R is the underlying statistical computing software. RStudio is a graphical integrated development environment (IDE) that makes using R much easier and more interactive. You need to install R before you install RStudio.\nThanks to the USGS-R Training group and Data Carpentry for making their installation materials available. The following instructions come directly from their materials, with a few minor edits to help you get set up.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Setup for the workshop</span>"
    ]
  },
  {
    "objectID": "setup.html#install-r-and-rstudio",
    "href": "setup.html#install-r-and-rstudio",
    "title": "Appendix A — Setup for the workshop",
    "section": "",
    "text": "A.1.1 Windows: Download and install R\nGo to CRAN and download the R installer for Windows. Make sure to choose the latest stable version (v4.4.1 as of August 2024).\nOnce the installer downloads, Right-click on it and select “Run as administrator”.\nType in your credentials and click yes (or if you don’t have administrator access have your IT rep install with Admin privileges).\n\nYou can click next through the standard dialogs and accept most defaults. But at the destination screen, please verify that it is installing it to C:\\Program Files\\R\n\nAt the “Select Components” screen, you can accept the default and install both 32-bit and 64-bit versions.\n\nAt this screen, uncheck ‘Create a desktop icon’ because non-admin users in Windows will be unable to delete it.\n\n\n\nA.1.2 Windows: Download and install RStudio\nDownload RStudio from here.\nAfter download, double-click the installer. It will ask for your administrator credentials to install (you might need to have your IT rep install again).\nAccept all the default options for the RStudio install.\n\n\n\nA.1.3 macOS: Download and install R\n\nDownload and install R from the CRAN website for Mac here.\nSelect the .pkg file for the latest R version\nDouble click on the downloaded file to install R\nIt is also a good idea to install XQuartz (needed by some packages)\n\n\n\nA.1.4 macOS: Download and install RStudio\n\nGo to the RStudio download page\nUnder Installers select the appropriate RStudio download file for macOS\nDouble click the file to install RStudio\n\n\n\nA.1.5 Check Install\nOnce installed, RStudio should be accessible from the start menu. Start up RStudio. Once running it should look something like this:",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Setup for the workshop</span>"
    ]
  },
  {
    "objectID": "setup.html#sec-instjags",
    "href": "setup.html#sec-instjags",
    "title": "Appendix A — Setup for the workshop",
    "section": "A.2 Install JAGS",
    "text": "A.2 Install JAGS\nThe JAGS software is a separate open-source program for analyzing Bayesian hierarchical models using Markov Chain Monte Carlo (MCMC) methods. It is used by EBASE to run the Bayesian models. Follow the instructions here to download and install the version appropriate for your operating system.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Setup for the workshop</span>"
    ]
  },
  {
    "objectID": "setup.html#sec-instpackages",
    "href": "setup.html#sec-instpackages",
    "title": "Appendix A — Setup for the workshop",
    "section": "A.3 Install R packages",
    "text": "A.3 Install R packages\nWe’ll use the following R packages during the workshop. Install them in the RStudio console by running these commands:\ninstall.packages('EBASE')\ninstall.packages('SWMPr')\ninstall.packages('tidyverse')\ninstall.packages('plotly')\ninstall.packages('here')\ninstall.packages('doParallel')\nAfter installation, check the packages can be loaded without error:\nlibrary(EBASE)\nlibrary(SWMPr)\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(here)\nlibrary(doParallel)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Setup for the workshop</span>"
    ]
  },
  {
    "objectID": "setup.html#sec-data",
    "href": "setup.html#sec-data",
    "title": "Appendix A — Setup for the workshop",
    "section": "A.4 Download the data",
    "text": "A.4 Download the data\nDownload the zipped data file from here: https://github.com/fawda123/ebase-training/raw/main/data/367272.zip\nKeep the file in a known location so that we can access it during the workshop.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Setup for the workshop</span>"
    ]
  },
  {
    "objectID": "setup.html#sec-cloud",
    "href": "setup.html#sec-cloud",
    "title": "Appendix A — Setup for the workshop",
    "section": "A.5 Posit Cloud (optional)",
    "text": "A.5 Posit Cloud (optional)\nPosit Cloud provides an environment to use RStudio and the resources above through a web browser. We’ve created a workspace on Posit Cloud that includes most all of the software and packages described above. Please only use this option as a last resort. We strongly encourage installing the software on your own computer.\nOpen the following URL in a web browser: https://posit.cloud/content/8518890\nYou will see a login screen that looks like this:\n\nSign up using a personal login or existing account (Google, GitHub, etc.).\nYou’ll see the workspace in your browser once you’ve signed in. You’ll need to make a permanent copy to save your work. Just click the button at the top marked “+ Save as Permanent Copy”. When this is done, the red text at the top indicating “TEMPORARY COPY” will no longer be visible.\n\nNow you can follow along with the workshop content.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Setup for the workshop</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Beck, M. W. 2016. SWMPr: An R package for\nretrieving, organizing, and analyzing environmental data for estuaries.\nR Journal 8: 219–232.\n\n\nBeck, M. W., J. M. Arriola, M. Herrmann, and R. G. Najjar. 2024. Fitting\nmetabolic models to dissolved oxygen data: The estuarine bayesian\nsingle-station estimation method. Limnology and Oceanography: Methods\n22: 590–607. doi:10.1002/lom3.10620\n\n\nBeck, M. W., J. D. Hagy III, and M. C. Murrell. 2015. Improving\nestimates of ecosystem metabolism by reducing effects of tidal advection\non dissolved oxygen time series. Limnology and Oceanography: Methods\n13: 731–745. doi:10.1002/lom3.10062\n\n\nCaffrey, J. M. 2004. Factors controlling net ecosystem metabolism in\nU.S. estuaries. Estuaries 27: 90–101. doi:10.1007/bf02803563\n\n\nHoellein, T. J., D. A. Bruesewitz, and D. C. Richardson. 2013.\nRevisiting Odum (1956): A synthesis of aquatic ecosystem\nmetabolism. Limnology and Oceanography 58: 2089–2100.\ndoi:10.4319/lo.2013.58.6.2089\n\n\nKemp, W. M., and J. M. Testa. 2011. Metabolic\nbalance between ecosystem production and consumption, p. 83–118.\nIn E. Wolanski and D. McLusky [eds.], Treatise on estuarine and\ncoastal science. Elsevier.\n\n\nMorel, A., and R. C. Smith. 1974. Relation between total quanta and\ntotal energy for aquatic photosynthesis. Limnology and Oceanography\n19: 591–600. doi:10.4319/lo.1974.19.4.0591\n\n\nOdum, H. T. 1956. Primary production in flowing waters. Limnology and\nOceanography 1: 102–117.\n\n\nSievert, C. 2020. Interactive web-based\ndata visualization with r, plotly, and shiny, Chapman; Hall/CRC.\n\n\nWanninkhof, R. 2014. Relationship between wind speed and gas exchange\nover the ocean revisited. Limnology and Oceanography: Methods\n12: 351–362. doi:10.4319/lom.2014.12.351",
    "crumbs": [
      "Appendices",
      "References"
    ]
  }
]