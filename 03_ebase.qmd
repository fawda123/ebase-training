---
lightbox: true

execute:
  echo: true
---

# Using EBASE {#sec-ebase}

## Lesson Outline

This lesson will explain the basic theory behind ecosystem metabolism and how EBASE is used to estimate key parameters.  EBASE is a Bayesian implementation for estimating metabolism that differs from more conventional approaches.  We'll also explore the main functions of EBASE to estimate metabolism, how the Bayesian approach can be used to incorporate prior knowledge, and interpret the results. 

## Learning Goals

- Understand the basic theories of EBASE
- Understand the Bayesian approach to estimating metabolism
- Learn how EBASE is used to estimate key parameters
- Use the main functions of EBASE to estimate metabolism
- Interpret the results of EBASE

## The EBASE method

[Lesson @sec-intro] provided a general description of ecosystem metabolism and why it's an important measure of ecosystem health.  The EBASE approach follows these general principles but differs in the core model for estimating production, respiration, and gas exchange and the statistical approach for how the parameters for each are estimated.  Let's revisit the core metabolic equation: 

$$ 
NEM = P - R
$$

Net ecosystem metabolism ($NEM$) is simply the difference between processes that produce organic matter (production or $P$) and those that consume organic matter (respiration or $R$).  

We can estimate these rates from the dissolved oxygen ($DO$) time series by using the rate of change of $DO$ per unit time and accounting for gas exchange $D$.  Water column depth ($Z$) is used to estimate metabolism as an areal rate. 

$$
Z\frac{dC_d}{dt} = P - R + D
$$

Methods for estimating metabolism from the dissolved oxygen time series vary in how each of the key parameters are modeled.  The core equation for EBASE is the same as the previous but expanded using methods for estuarine applications: 

$$
Z\frac{dC_d}{dt} = aPAR - R + bU_{10}^2\left(\frac{Sc}{600} \right)^{-0.5} \left(C_{Sat} - C_d \right )
$$

$aPAR$ is production ($P$), $R$ is respiration, and the last term is gas exchange [$D$, @Wanninkhof14].  The required input data to fit this model was described in [Lesson @sec-dataprep].  Three of these inputs are used directly in the equation as $C_d$ for dissolved oxygen, $PAR$, and wind speed as $U_{10}^2$ (or squared wind speed at ten meters above the surface).  The other terms are the Schmidt number ($S_c$) and dissolved oxygen at saturation ($C_{sat}$), both of which are calculated from water temperature and salinity in the input data. 

The remaining terms $a$, $R$, and $b$ are estimated during the model fitting process, which we'll discuss in the next section.  All three provide important information about metabolic processes and are useful on their own to understand ecosystem function.

* $a$: The light efficiency parameter as $\left(mmol~O_2~m^{-2}~d^{-1}\right) / \left(W~m^{-2}\right)$, yields production when multiplied by $PAR$.  This is a measure of how efficiently light is converted to organic matter.  It provides similar information as a P/I curve by showing how production changes with light availability.
* $R$: The respiration rate as $mmol~O_2~m^{-2}~d^{-1}$, is the rate at which organic matter is consumed.
* $b$: The sensitivity of gas exchange to wind speed as $\left(cm~h^{-1}\right) / \left(m^2~s^{-2}\right)$.

Although the core model equation may seem complicated, its formulation was chosen to describe dominant processes that influence metabolism in an estuarine setting.  The details for this justification can be found in @Beck2024.

## The Bayesian approach

Both the open-water method presented by @Odum56 and EBASE similarly use the rate of change of $DO$ to estimate metabolism.  However, the Bayesian approach used in EBASE differs in how the parameters are estimated and has several key advantages.  

Modern statistical approaches to modelling can broadly be described as conventional "frequentist" methods or Bayesian.  The former describes standard and more commonly used methods where a model is fit to a dataset to support a hypothesis, as in a simple linear regression.  Bayesian approaches turn the paradigm around by asking the question "what is the likelihood of the data given a model?".  

Bayesian approaches are fundamentally linked to Bayes theorem that describes the likelihood of observing an event as depending on the likelihood that other events have occurred. In model speak, this means we can estimate the posterior distribution of a parameter given a prior distribution.  For EBASE, we can establish a set of prior distributions of likely values for each of the unknown parameters, $a$, $R$, and $b$.  These prior distributions are used to create a refined estimate for the posterior distributions given the data that is presented to the model. The prior distributions can be precise (informed) or cover a range of potential values (uninformed).  

The default prior distributions for EBASE were based on a likely range of values for each parameter that were informed by published results [primarily @caffrey2004; details in @Beck2024].  You can create a plot of these prior distributions using the `prior_plot` function in EBASE. 

```{r}
#| fig-height: 3
#| fig-width: 8
library(EBASE)

prior_plot()
```

Running EBASE with the default arguments will use these prior distributions to estimate the parameters.  We'll discuss how to change these values later in the lesson if you have additional prior information. 

Finally, the Bayesian approach in EBASE uses an MCMC (Markov Chain Monte Carlo) method to estimate the posterior distributions.  This is implemented using the [JAGS](https://mcmc-jags.sourceforge.io/) software which was part of the installation during [setup](setup.qmd#sec-instjags) for the workshop.  These methods are used to estimate the posterior distributions by repeated sampling of the priors given the data until a stable estimate is reached (i.e., convergence) using a Markov Chain.  Because of this, estimation can take a while and we'll talk about arguments in EBASE to control the process.  

## EBASE minimal example

Below is a minimal example of how to use EBASE with the example dataset provided in the package. We'll subset only a few days so that we can run the example in real time.  Some helper functions from dplyr are used to subset the data before using EBASE. 

```{r}
library(dplyr)

# dates to subset
dts <- as.POSIXct(c("2012-06-01 00:00:00", "2012-06-05 00:00:00"), tz = 'America/Jamaica')

# subset exdat
dat <- exdat |> 
  filter(DateTimeStamp >= dts[1] & DateTimeStamp <= dts[2])

head(dat)
```

Now we run EBASE using the data subset and the default arguments.  We must supply values for the `interval` argument that defines the time step of the data seconds and the water column depth `Z` in meters, which we can get from metadata. 

```{r}
# run ebase
res <- ebase(dat, interval = 900, Z = 1.85)

head(res)
```

The results are returned as a data frame with instantaneous metabolic estimates for areal gross production (O2 mmol m$^{-2}$ d$^{-1}$, `P`), respiration (O2 mmol m$^{-2}$ d$^{-1}$, `R`), and gas exchange (O2 mmol m$^{-2}$ d$^{-1}$, `D` or the remainder of the equation from above, positive values as ingassing, negative values as outgassing).  

Additional parameters estimated by the model that are returned include `a` as (mmol O$_2$ m$^{-2}$ d$^{-1}$)/(W m$^{-2}$) and `b` as (cm h$^{-1}$)/(m$^2$ s$^{-2}$). We also see modelled DO as `DO_mod` and various convergence metrics.  The upper and lower credible intervals for each estimated parameter are also provided by `lo` or `hi` suffixes, which we'll describe later to assess model fit. 

::: {.callout-note icon="false"}
### `r fontawesome::fa('hat-wizard')` Exercise 1
Repeat the above example but use a different date range.

1. Create and name a section header in your script with `Ctrl + Shift + R`. Enter all exercise code in this section.
1. Create another date range of similar length (four days).
1. Subset the `exdat` data to this new date range.
1. Run `ebase` on the new data subset.  
1. Examine the results - what do the columns tell you? 

::: {.callout-tip icon="false" collapse="true"}
#### `r fontawesome::fa('wand-magic-sparkles')` Answers
```{r}
#| eval: false
# dates to subset
dts <- as.POSIXct(c("2012-07-01 00:00:00", "2012-07-05 00:00:00"), tz = 'America/Jamaica')

# subset exdat
dat <- exdat |> 
  filter(DateTimeStamp >= dts[1] & DateTimeStamp <= dts[2])

# run ebase
res <- ebase(dat, interval = 900, Z = 1.85)

head(res)
```
:::
:::

## EBASE full example

Next we'll use EBASE to estimate metabolism using the dataset from [Lesson @sec-dataprep].  First, the dataset is loaded into your workspace. 

```{r}
load(file = 'data/apadb.RData')
```

As with the example dataset, we'll subset a few days so we can run the example in real time.  We'll use one week of data in July. 

```{r}
# dates to subset
dts <- as.POSIXct(c("2017-03-07 00:00:00", "2017-03-14 00:00:00"), tz = 'America/Jamaica')

# subset apadb
apadbsub <- apadb |> 
  filter(DateTimeStamp >= dts[1] & DateTimeStamp <= dts[2])

head(apadbsub)
```

Now we run EBASE as before, although we have to specify different values for the `interval` and `Z` arguments.  Because we combined the data at a 60 minute time step, we can use `interval = 3600`.  The water column depth can be estimated from the data.  We'll also add a small amount to it because the pressure sensor is typically offset from the bottom.

```{r}
#| warning: true
#| message: true
# get depth
zval <- mean(apadbsub$Depth, na.rm = T) + 0.3

# run ebase
res <- ebase(apadbsub, interval = 3600, Z = zval)
```

First off, we got a few warnings in the console telling us some information about our dataset.  Both of these warnings indicate the default behavior for EBASE when incomplete daily observations at the start or end of the dataset are present and if missing observations are found.  As the warnings indicate, the "dangling" observations are removed since it is impossible to estimate metabolism accurately for these data.  Missing data in the middle of the time series are also interpolated and these can be handled differently depending on the user's preference.  We'll describe more about this in @sec-missing. 

We'll use some additional functions in EBASE to interpret the results. A plot of the results can be made with `ebase_plot()`.  This plot shows $P$, $R$, and $D$ as instantaneous values at the same time step as the original dataset. Note that a single value for $R$ is returned each day.  The model assumes respiration is constant for each period of time the model is estimated.  This is called the "optimization period" and it is set as 1 day by default.  The `grp` column in the model output indicates which optimization period applies to each observation.  We'll talk more about how and why you might change this value in @sec-optimization.  

```{r}
#| fig-height: 3
#| fig-width: 9
ebase_plot(res)
```

The daily averages can also be plotted by using `instantaneous = FALSE`.

```{r}
#| fig-height: 3
#| fig-width: 9
ebase_plot(res, instantaneous = FALSE)
```

Note that $NEM$ is not returned by `ebase`, nor is it included in the plots.  We can easily calculate $NEM$ by subtracting $R$ from $P$ and use the ggplot2 package to plot the result. 

```{r}
#| fig-height: 3
#| fig-width: 9
# calculate NEM
res <- res |> 
  mutate(
    NEM = P - R
  )

# make a plot
library(ggplot2)

ggplot(res, aes(x = DateTimeStamp, y = NEM)) +
  geom_line() +
  geom_hline(yintercept = 0, linetype = 'dashed') +
  theme_minimal() + 
  labs(
    x = NULL,
    y = 'NEM (mmol O2 m-2 d-1)'
  )
```

Or we can average NEM each day and plot it again. 

```{r}
#| fig-height: 3
#| fig-width: 9
# calculate daily NEM
nemdly <- res |> 
  summarise(
    NEM = mean(NEM, na.rm = T), 
    .by = Date
  )

# plot daily NEM
ggplot(nemdly, aes(x = Date, y = NEM)) +
  geom_line() +
  geom_hline(yintercept = 0, linetype = 'dashed') +
  theme_minimal() +
  labs(
    x = NULL,
    y = 'NEM (mmol O2 m-2 d-1)'
  )
```

## Assess model fit

Model fit can be assessed using the `converge` and `rsq` columns from the returned results.  The values in these columns apply to each group in the `grp` column as specified with the `ndays` argument. The `converge` column indicates `"Check convergence"` or `"Fine"` if the JAGS estimate converged at that iteration or optimization period (repeated across rows for the group). Similarly, the `rsq` column shows the r-squared values of the linear fit between the modeled and observed dissolved oxygen (repeated across rows for the group).  

The model fit can also be assessed by comparing the observed and modeled values for dissolved oxygen with the `fit_plot()` function.  Estimated values are shown as lines and observed values are shown as points.

```{r}
#| fig-height: 3
#| fig-width: 9
fit_plot(res)
```

The comparison can also be separated by group with `bygroup = TRUE` based on the value for the `ndays` argument passed to `ebase()`, default as 1 day.  The r-squared value of the fit between modeled and observed dissolved oxygen is also shown in the facet label for the group.

```{r}
#| fig-height: 7
#| fig-width: 9
fit_plot(res, bygroup = TRUE)
```

A scatterplot showing modeled versus observed dissolved oxygen can also be returned by setting `scatter = TRUE`.

```{r}
#| fig-height: 7
#| fig-width: 9
fit_plot(res, bygroup = TRUE, scatter = TRUE)
```

95% credible intervals for `a`, `R`, and `b` are also returned with the output from `ebase()` in the corresponding columns `alo`, `ahi`, `blo`, `bhi`, `Rlo`, and `Rhi`, for the 2.5th and 97.5th percentile estimates for each parameter, respectively.  These values indicate the interval within which there is a 95% probability that the true parameter is in this range and is a representation of the posterior distributions for each parameter. 

The credible intervals can be plotted with the `credible_plot()` function.

```{r}
#| fig-height: 7
#| fig-width: 9
credible_plot(res)
```

The credible intervals can also be retrieved as a data frame using `credible_prep()`.  This function is provided as a convenience to parse the results from `ebase()`. 

```{r}
credible_prep(res)
```

## Using EBASE with long time series

Execution time of the model can also be reduced by using multiple processors.  This is done using the [doParrallel](https://cran.r-project.org/package=doParallel) package and creating a parallel backend as below. It doesn't really help us when the time series is short, but it can be useful when running more than a few days of data.

```{r}
#| eval: false
# setup parallel backend
library(doParallel)
ncores <- detectCores() - 2
cl <- makeCluster(ncores)
registerDoParallel(cl)

res <- ebase(apadbsub, interval = 3600, Z = zval)

stopCluster(cl)
```

Finally, although `ebase()` can be used to estimate metabolism for time series with several years of data, the `ebase_years()` function can be used to estimate results sequentially for each year.  This is useful because model estimation using `ebase_years()` will continue after a year fails, e.g., when some years have long periods of missing or erroneous data.  This eliminates the need to restart the model or further pre-process the data.  The same arguments for `ebase()` are used for `ebase_years()`.  Progress is printed directly in the console and the user can specify the number of attempts for failed years before proceeding to the following year. 

```{r}
#| eval: false
zval <- mean(apadb$Depth, na.rm = T)
resfull <- ebase_years(apadb, Z = zval, interval = 3600, ncores = ncores, quiet = F)
save(resfull, file = 'data/resfull.RData')
```

## Equation optimization length {#sec-optimization}

The `ndays` argument in `ebase()` defines the model optimization period as the number of days that are used for fitting the model.  By default, this is done each day, i.e., `ndays = 1`.  Individual parameter estimates for `a`, `R`, and `b` are then returned for each day.  However, more days can be used to estimate the unknown parameters - one day may not provide enough data to the model to create a reliable estimate. 

Here, the number of days used to optimize the equation is set to all days in the input data. 

```{r}
res <- ebase(apadbsub, interval = 3600, Z = zval, ndays = 7)
```

And the resulting plot: 

```{r, fig.height = 3, fig.width = 9}
#| fig-height: 3
#| fig-width: 9
ebase_plot(res)
```

And the fit of observed and modeled dissolved oxygen (note the unbroken line for all days estimated together): 

```{r}
#| fig-height: 3
#| fig-width: 9
fit_plot(res)
```

Choosing the number of days to optimize the equation is a user choice based on questions of interest for the dataset.  Typically, using one day for optimization may not provide enough data to meaningfully describe metabolism, although this may be the only option for very short time series.  Longer optimization periods can be used for longer time series, but key events driving metabolism may be missed if the optimization period is too long, e.g., monthly. 

::: {.callout-note icon="false"}
### `r fontawesome::fa('hat-wizard')` Exercise 2
Run `ebase` on a different data subset, change the optimization period, and examine the results.

1. Create and name a section header in your script with `Ctrl + Shift + R`. Enter all exercise code in this section.
1. Create another date range of similar length for 2017.
1. Subset the `apadb` data to this new date range.
1. Estimate average depth from the data subset.
1. Run `ebase` on the new data subset using a different optimization length for the `ndays` argument. Make sure to also pass the `Z` argument with the average depth.
1. Examine the results using `fit_plot`, `ebase_plot`, and `credible_plot`.

::: {.callout-tip icon="false" collapse="true"}
#### `r fontawesome::fa('wand-magic-sparkles')` Answers
```{r}
#| eval: false
# dates to subset
dts <- as.POSIXct(c("2017-08-01 00:00:00", "2017-08-08 00:00:00"), tz = 'America/Jamaica')

# subset exdat
apadbsub <- apadb |> 
  filter(DateTimeStamp >= dts[1] & DateTimeStamp <= dts[2])

# get average depth
zval <- mean(apadbsub$Depth, na.rm = T)

# run ebase
res <- ebase(apadbsub, interval = 3600, Z = zval, ndays = 2)

fit_plot(res)
ebase_plot(res)
credible_plot(res)
```
:::
:::

## Missing values {#sec-missing}

Missing values in the input data are interpolated by `ebase` prior to estimating metabolism.  It is the responsibility of the user to verify that these interpolated values are not wildly inaccurate.  Missing values are linearly interpolated between non-missing values at the time step specified by the value in `interval`.  This works well for small gaps, but can easily create inaccurate values at gaps larger than a few hours. 

Let's create some missing data in our Apalachicola subset.  We'll remove a few hours on one day and remove an entire day.

```{r}
apadbmiss <- apadbsub
apadbmiss$DO_obs[c(57:61, 97:120)] <- NA
```

The interpolated values can be visually inspected using the `interp_plot()` function.  

```{r}
#| fig-height: 3
#| fig-width: 9
interp_plot(apadbmiss, Z = zval, interval = 3600, param = 'DO_obs')
```

The `ebase` function includes the `maxinterp` argument to assign `NA` values to continuously interpolated rows with length greater than the value defined by `maxinterp`.  This value is set to 12 hours by default and applies to the groupings defined by `ndays`, i.e., any group with a continuous set of interpolated values where the time is greater than 12 hours are assigned `NA` (except `Date` and `DateTimeStamp`).  The numeric value passed to `maxinterp` is the number of time steps for the input data.

Running `ebase` on the time series with missing data shows how the interpolatd data are handled.  Note that results for the long gap are removed, whereas those for the shorter gaps are retained. 

```{r}
#| fig-height: 3
#| fig-width: 9
res <- ebase(apadbmiss, interval = 3600, Z = zval)
ebase_plot(res)
```

## Changing priors

A main advantage of the Bayesian approach is the use of prior information to estimate parameters.  By default, the prior distributions for the $a$, $R$, and $b$ parameters are informed by the literature, although they are generally uninformed by allowing a large range of values to be considered given the data.  We can further constrain the parameters if we have reason to do so.

Here, the prior distribution for the $b$ parameter is fixed to 0.251 (cm h$^{-1}$)/(m$^2$ s$^{-2}$), as suggested by @Wanninkhof2024.  We can view this change with the `prior_plot` function before using `ebase`. Note that we define the mean as our desired value and assign a very small range for the standard deviation.

```{r}
#| fig-height: 3
#| fig-width: 9
prior_plot(bprior = c(0.251, 1e-6))
```

The same change to the prior distribution for the $b$ parameter is applied to `ebase`

```{r}
#| fig-height: 3
#| fig-width: 9
res <- ebase(apadbsub, interval = 3600, Z = zval, bprior = c(0.251, 1e-6))
ebase_plot(res)
```

The `credible_plot` function can be used to assess how changing the prior distributions has an influence on the posterior distributions of the parameters.

```{r}
#| fig-height: 7
#| fig-width: 9
credible_plot(res)
```

::: {.callout-note icon="false"}
### `r fontawesome::fa('hat-wizard')` Exercise 3
Run `ebase` using a different set of prior distributions.

1. Create and name a section header in your script with `Ctrl + Shift + R`. Enter all exercise code in this section.
1. Use the same data subset and mean depth as the previous exercise.
1. Change the prior distribution for the $a$ parameter to a mean of 1 and a standard deviation of 1 and the prior distribution for the $b$ parameter to a mean of 0.251 and a standard deviation of 1e-6.
1. Before running `ebase`, evaluate the prior distribution using `prior_plot`.
1. Run `ebase` using the new prior distribution. Use `ndays = 2`. 
1. Examine the results using `fit_plot`, `ebase_plot`, and `credible_plot`.

::: {.callout-tip icon="false" collapse="true"}
#### `r fontawesome::fa('wand-magic-sparkles')` Answers
```{r}
#| eval: false
# dates to subset
dts <- as.POSIXct(c("2017-08-01 00:00:00", "2017-08-08 00:00:00"), tz = 'America/Jamaica')

# subset exdat
apadbsub <- apadb |> 
  filter(DateTimeStamp >= dts[1] & DateTimeStamp <= dts[2])

# get average depth
zval <- mean(apadbsub$Depth, na.rm = T)

# use prior_plot
prior_plot(aprior = c(1, 1), bprior = c(0.251, 1e-6))

# run ebase
res <- ebase(apadbsub, interval = 3600, Z = zval, aprior = c(1, 1), bprior = c(0.251, 1e-6), ndays = 2)

fit_plot(res)
ebase_plot(res)
credible_plot(res)
```
:::
:::

## Next steps

You now understand the basics of ecosystem metabolism with EBASE and how it estimates key parameters using a Bayesian approach.  We've explored additional functions in EBASE to help interpret the results, which include an evaluation of goodness of fit and various plotting methods.  Next, we'll explore the EBASE results in more detail to demonstrate how the results can inform the understanding of ecosystem health and function.