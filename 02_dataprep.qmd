---
lightbox: true

execute:
  echo: true
---

# Data Preparation {#sec-dataprep}

## Lesson Outline

The EBASE software requires a specific format for input data that includes both water quality and weather data. This lesson will demonstrate how the SWMPr package can be used to import and prepare data from the NERRS System Wide Monitoring Program (SWMP) for analysis.  We'll also cover how to prepare non-SWMP data using R packages from the tidyverse.

## Learning Goals

- Understand the data requirements for EBASE
- Learn how to import data with SWMPr
- Learn how to clean and combine SWMP data
- Learn how to prepare data for EBASE analysis with other tools

## Data requirements for EBASE {#sec-ebasedata}

The metabolism functions in EBASE require both water quality and weather data.  Both are assumed continuous where sufficient observations are collected each day to describe the diel cycling of dissolved oxygen and other required parameters.  Because water quality and weather data are collected by different sensors, these data typically are not provided in the same file.  A bit of preprocessing is needed to combine the data before EBASE can be used.  

We'll start by loading EBASE and viewing an example file that's included with the package.  This example file is used for all the documentation in EBASE and you can use it to learn how the main functions work. We'll use other data files below.

```{r}
library(EBASE)
```

View the first few rows of the example data file:

```{r}
head(exdat)
```

Check the structure of the example data file:

```{r}
str(exdat)
```

Only five parameters are required to use EBASE, where each row represents a single observation indexed by ascending time. 

* `DateTimeStamp`: Date and time of the observation, as a POSIXct object with an appropriate time zone. 
* `DO_obs`: Observed dissolved oxygen concentration in mg/L.
* `Temp`: Water temperature in degrees Celsius.
* `Sal`: Salinity in psu.
* `PAR`: Total photosynthetically active radiation in Watts per square meter.
* `WSpd`: Wind speed in meters per second.

The dissolved oxygen, water temperature, and salinity data are typically collected by a water quality sonde, while the PAR and wind speed data are typically collected at a weather station.  We'll discuss why these parameters are needed to calculate metabolism in [Lesson @sec-ebase]. These data must be combined into a single data frame for EBASE to work.  We'll do this in the next few sections. 

## Data import with SWMPr

The [SWMPr](https://cran.r-project.org/web/packages/SWMPr/index.html) package was developed in 2016 to provide a bridge between the raw data from the NERR System Wide Monitoring Program (SWMP) network and the R analysis platform [@Beck2016]. It does this with some degree of proficiency, but by far it's most useful feature is being able to import and combine time series from SWMP sites with relative ease.  In particular, the `import_local`, `qaqc`, and `comb` functions allow us to quickly import, clean up, and combine datasets for follow-up analysis.  This is all we'll do with SWMPr in these lessons.

The `import_local` function is designed to work with SWMP data downloaded from the Centralized Data Management Office (CDMO) using the [Zip Downloads](http://cdmo.baruch.sc.edu/aqs/zips.cfm) feature from the Advanced Query System.  The files we have in our "data" folder were requested from this feature for Apalachicola Bay data from 2017 to 2019.

The `import_local` function has two arguments: `path` to indicate where the data are located and `station_code` to indicate which station to import.  We first load SWMPr and then use the function to import data for the Apalachicola Dry Bar station:

```{r}
# load SWMPr
library(SWMPr)

# import data
apadbwq <- import_local(path = 'data/367272.zip', station_code = 'apadbwq')

# characteristics of the dataset
head(apadbwq)
dim(apadbwq)
range(apadbwq$datetimestamp)
```

Note that this function was able to import and combine data from multiple csv files.  We would have had to do this by hand if we were importing data with more general import functions available in R (e.g., `read.csv`).

## Cleaning and combining SWMP data

Each row has data for multiple parameters at 15 minute intervals.  Each parameter also includes a column with [QAQC flags](http://cdmo.baruch.sc.edu/data/qaqc.cfm), i.e., `f_` then the parameter name.  We can use the `qaqc` function to "screen" observations with specific QAQC flags. We'll keep all observations that have the flags 0, 1, 2, 3, 4, and 5 by indicating this information in the `qaqc_keep` argument (in practice, you may only want to keep data with a "zero" flag). You can view a tabular summary of the flags in a dataset using the `qaqcchk` function.

```{r}
# keep only observations that passed qaqc chekcs
apadbwq <- qaqc(apadbwq, qaqc_keep = c('0', '1', '2', '3', '4', '5'))

# check the results
head(apadbwq)
dim(apadbwq)
range(apadbwq$datetimestamp)
```

Notice that the number of rows are the same as before - no rows are removed by `qaqc`.  Values that did not fit the screening criteria are given a `NA` value.  Also notice the flag columns are removed.

The EBASE functions also require weather data.  We can repeat the steps above to import and clean data from the weather station at Apalachicola.

```{r}
# import weather data, clean it up
apaebmet <- import_local(path = 'data/367272.zip', station_code = 'apaebmet')
apaebmet <- qaqc(apaebmet, qaqc_keep = c('0', '1', '2', '3', '4', '5'))

# check the results
head(apaebmet)
dim(apaebmet)
range(apaebmet$datetimestamp)
```

The `comb` function in SWMPr lets us combine data from two locations using the `datetimestamp` column.  We need to do this to use the functions in EBASE that require both water quality and weather data.

There are a couple of arguments to consider for the `comb` function.  First, the `timestep` argument defines the time step for the resulting output.  Keep this at 15 to retain all of the data.  You could use a larger time step to subset the data if, for example, we wanted data every 60 minutes. Second, the `method` argument defines how two datasets with different date ranges are combined.  Use `method = 'union'` to retain the entire date range across both datasets or use `method = 'intersect'` to retain only the dates that include data from both datasets.  For our example, `union` and `intersect` produce the same results since the date ranges and time steps are the same.

To speed up the examples in our lesson, we'll use a 60 minute timestep.  In practice, it's better to retain all of the data (i.e., `timestep = 15`).

```{r}
# combine water quality and weather data
apadb <- comb(apadbwq, apaebmet, timestep = 60, method = 'union')

# check the results
head(apadb)
dim(apadb)
range(apadb$datetimestamp)
```

::: {.callout-note icon="false"}
### `r fontawesome::fa('hat-wizard')` Exercise 1
Repeat the above examples but do this using data for the East Bay station at Apalachicola.  Import data for `apaebwq` and `abaebmet`, clean them up with `qaqc`, and combine them with `comb`.

1. Create and name a section header in your script with `Ctrl + Shift + R`. Enter all exercise code in this section.
1. Load the SWMPr package with the `library` function.  This should already be installed from last time (i.e., `install.packages('SWMPr')`).
1. Import and clean up `apaebwq` with `import_local` and `qaqc`.
1. Import and clean up `apaebmet` with `import_local` and `qaqc`.
1. Combine the two with `comb`. Use a 60 minute time step and use the `union` option.

::: {.callout-tip icon="false" collapse="true"}
#### `r fontawesome::fa('wand-magic-sparkles')` Answers
```{r}
# import water quality data, clean it up
apaebwq <- import_local(path = 'data/367272.zip', station_code = 'apaebwq')
apaebwq <- qaqc(apaebwq, qaqc_keep = c('0', '1', '2', '3', '4', '5'))

# import weather data, clean it up
apaebmet <- import_local(path = 'data/367272.zip', station_code = 'apaebmet')
apaebmet <- qaqc(apaebmet, qaqc_keep = c('0', '1', '2', '3', '4', '5'))

# combine water quality and weather data
apaeb <- comb(apaebwq, apaebmet, timestep = 60, method = 'union')
```
:::
:::

## Preparing SWMP data for EBASE

Now we want to setup our data for use with functions in the EBASE package.  As with most R functions, the input formats are very specific requiring us to make sure the column names, locations, and types of columns in our data are exactly as needed.  

The example dataset from EBASE can be used to guide us in preparing the data. We can compare this to our Apalachicola dataset from above.

```{r}
# view first six rows of example data
head(exdat)

# view the structure of example data
str(exdat)

# view first six rows of apadb data
head(apadb)

# view the structure of apadb data
str(apadb)
```

So, we need to do a few things to our Apalachicola dataset to match the format of the `exdat` dataset. We can use the dplyr package to "wrangle" the data into the correct format (here's a useful [cheatsheet](https://rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf) for this package).  The dplyr package comes with the tidyverse.

All we need to do is rename the columns and select those we want in the correct order.  This can all be done with the `select` function in dplyr.

```{r}
# load dplyr
library(dplyr)

# select and rename columns
apadb <- select(apadb,
  DateTimeStamp = datetimestamp,
  DO_obs = do_mgl,
  Temp = temp,
  Sal = sal,
  PAR = totpar,
  WSpd = wspd,
)

# show first six rows
head(apadb)

# view structure
str(apadb)
```

We can also verify the column names are the same between the two datasets.  Note the use of two equal signs - this is how we tell R to test for equality.

```{r}
names(apadb) == names(exdat)
```

The last thing we need to do is to make sure the units are correct. Looking at the requirements in @sec-ebasedata and those from the [CDMO](https://cdmo.baruch.sc.edu/data/parameters.cfm) website, all of the data are in the correct units except for PAR.  This should be in Watts per square meter, whereas the data are millimoles per square meter per 15 minute logging interval.  @morel1974 provide the following conversion factor:

$$
1 \, \mu mol \, m^{-2} \, s^{-1} = 0.2175 \, W \, m^{-2}
$$
Before we apply the conversion factor, we need to convert the units of PAR from millimoles to micromoles and from per 15 minutes to per second.  This can be done by simply multiply PAR by 1000 (milli to micro) and dividing by 900 (15 minutes = 900 seconds).  The final conversion will look like this:

$$
1 \, W \, m^{-2} = 1 \, mmol \, m^{-2} \, 15min^{-1} * 1000 / 900 * 0.2175
$$

We can use the `mutate` function in dplyr to convert this.

```{r}
# convert PAR to Watts per square meter
apadb <- apadb |> 
  mutate(
    PAR = PAR * 1000 / 900 * 0.2175
  )
```

The Apalachicola data should now work with EBASE. However, a few additional exploratory analyses are worthwhile before we use these data.  Most functions in R deal with missing data by either not working at all or providing some work-around to accommodate the missing values. The functions in EBASE are in the latter category and we'll discuss how this is done in [Lesson @sec-ebase].  However, knowing how much and where missing observations occur is still important for interpreting results. 

Missing data in SWMP are not uncommon and they often occur when sondes or other equipment are down for maintenance or are otherwise broken for a period of time.  Missing observations usually come in blocks where all parameters are unavailable, as opposed to only one parameter. Below, we can quickly see how many missing observations occur in column.

```{r}
apply(apadb, 2, function(x) sum(is.na(x)))
```

There are quite a few missing observations.  Let's create some quick plots to see where these occur. We'll use the [plotly](https://plotly.com/r/) R package that lets us dynamically interact with the data [@sievert2020].

```{r}
library(plotly)

plot_ly(apadb, x = ~DateTimeStamp, y = ~DO_obs, type = 'scatter', mode = 'lines')
```

There's a huge gap in the fall of 2018 and the latter half of 2019.  Using these years to estimate metabolism may not be advisable. 

Similarly, the weather data also have missing observations. What may have caused this gap?

```{r}
plot_ly(apadb, x = ~DateTimeStamp, y = ~WSpd, type = 'scatter', mode = 'lines')
```

This brief assessment can give us information on which years of data are useful to interpret for metabolism.  A more detailed analysis of the quality of the data is needed before more formal analyses, including an assessment of the QC codes in the SWMP data.  This quick assessment will suffice for now.

::: {.callout-note icon="false"}
## `r fontawesome::fa('hat-wizard')`: Exercise 2

Repeat the above examples but use the combined dataset for the Apalachicola Bay East Bay data that you created in Exercise 1.

1. Create a new section in your script using `Ctrl + Shift + R` and give it an appropriate name.
1. Load the dplyr package with the library function.
1. Simultaneously rename and select the columns for date/time (`DateTimeStamp = datetimestamp`), dissolved oxygen (`DO_obs = do_mgl`), water temperature (`Temp = temp`), salinity (`Sal = sal`), PAR (`PAR = par`), and wind speed (`WSpd = wspd`) with the `select` function from dplyr. Donâ€™t forget to assign the new dataset to an object in your workspace (with <-).
1. Convert PAR to the correct units.

::: {.callout-tip icon="false" collapse="true"}
### `r fontawesome::fa('wand-magic-sparkles')` Answers
```{r}
# select and rename columns
apaeb <- select(apaeb,
  DateTimeStamp = datetimestamp,
  DO_obs = do_mgl,
  Temp = temp,
  Sal = sal,
  PAR = totpar,
  WSpd = wspd,
)

# convert PAR
apaeb <- apaeb |> 
  mutate(
    PAR = PAR * 1000 / 900 * 0.2175
  )
```
:::
:::

## Preparing other data

Long-term continuous monitoring data from other sources can be used with EBASE. As above, the data must include relevant water quality and weather data at an appropriate time step.  This section will show you how to prepare generic data with more general tools in R.

Data from Tampa Bay will be used in this example. We'll pull water quality data from a continuous monitoring platform in the lower bay and weather data from a nearby NOAA monitoring station.  The data are available [here](http://tampabay.loboviz.com/) and can be downloaded through their [API](http://tampabay.loboviz.com/cgidoc/). Take a look at the URL to see how the API is queried. 

```{r}
url <- 'http://tampabay.loboviz.com/cgi-data/nph-data.cgi?node=82&min_date=20210701&max_date=20210801&y=salinity,temperature,oxygen,par&data_format=text'

lobo <- read.table(url, skip = 2, sep = '\t', header = T)
head(lobo)
str(lobo)
```

The data are not in the right format for EBASE. We need to change the names, convert the date/time column to a POSIXct object with the correct time zone, and convert the PAR data to the correct units.  PAR is micromoles per square meter per second, so we need to convert this to Watts per square meter.  First, we select and rename the columns.

```{r}
lobo <- lobo |> 
  select(
    DateTimeStamp = date..EST.,
    DO_obs = dissolved.oxygen..mg.L.,
    Temp = temperature..C.,
    Sal = salinity..PSU.,
    PAR = PAR..uM.m.2.sec.
  )
```

Next, we use mutate to convert the date/time column to a POSIXct object and the PAR data to the correct units. For date/time, we can use a function from the lubridate package and use the "America/Jamaica" time zone, which is eastern time without daylight savings.  Note that this step is not needed for SWMP data since the SWMPr package automatically converts the date/time column using the correct time zone. The PAR column is converted using the same formula from above, except we don't need to convert from millimoles to micromoles and from 15 minutes to seconds. 

```{r}
lobo <- lobo |> 
  mutate(
    DateTimeStamp = lubridate::ymd_hms(DateTimeStamp, tz = 'America/Jamaica'),
    PAR = PAR * 0.2175
  )
```

Next we import the weather data.  These can be similarly downloaded using the NOAA Tides & Currents [API](https://api.tidesandcurrents.noaa.gov/api/prod/#units).  Again, note how the URL is specified to get the data we want.

```{r}
url <- 'https://api.tidesandcurrents.noaa.gov/api/prod/datagetter?product=wind&application=NOS.COOPS.TAC.MET&begin_date=20210701&end_date=20210801&station=8726520&time_zone=LST&units=metric&format=CSV'

ports <- read.table(url, sep = ',', header = T)
head(ports)
str(ports)
```

Similar to the water quality data, we need to select and rename the columns and convert the date/time column to a POSIXct object with the correct time zone.  Instead of `ymd_hms()` as above, we'll use `ymd_hm()` because seconds are not included. We'll do this in one step.

```{r}
ports <- ports |> 
  select(
    DateTimeStamp = Date.Time,
    WSpd = Speed
  ) |> 
  mutate(
    DateTimeStamp = lubridate::ymd_hm(DateTimeStamp, tz = 'America/Jamaica')
  )
```

The last step is to combine the water quality and weather data.  We can use a simple join function from the dplyr package.  The water quality and weather data are at different time steps, where the water quality data is every hour on the 30 minute mark and the weather data are every six minutes.  Because the weather data also include observations on the 30 minute mark, we can just left join them to the water quality data, retaining only those data on the 30 minute mark. See [here](https://r4ds.hadley.nz/joins) for more information about joins.

```{r}
tbdat <- left_join(lobo, ports, by = 'DateTimeStamp')

head(tbdat)
str(tbdat)
```

We see that the final dataset has the same number of rows as the water quality data and a wind speed column has been added from the weather data. Now the data are ready for analysis. 

::: {.callout-note icon="false"}
## `r fontawesome::fa('hat-wizard')`: Exercise 2

We'll want to visually evaluate the Tampa Bay data before using EBASE.  

1. Create and name a section header in your script with `Ctrl + Shift + R`. Enter all exercise code in this section.
1. Load the plotly package with the `library` function.
1. Use the plotly package to create some time series plots for variables of interest.  
1. Look for gaps or outliers. 
1. Look for interesting trends that might influence how you interpret metabolism.

::: {.callout-tip icon="false" collapse="true"}
### `r fontawesome::fa('wand-magic-sparkles')` Answers
```{r}
#| results: 'hide'
library(plotly)

plot_ly(tbdat, x = ~DateTimeStamp, y = ~DO_obs, type = 'scatter', mode = 'lines')
plot_ly(tbdat, x = ~DateTimeStamp, y = ~Temp, type = 'scatter', mode = 'lines')
plot_ly(tbdat, x = ~DateTimeStamp, y = ~Sal, type = 'scatter', mode = 'lines')
plot_ly(tbdat, x = ~DateTimeStamp, y = ~PAR, type = 'scatter', mode = 'lines')
plot_ly(tbdat, x = ~DateTimeStamp, y = ~WSpd, type = 'scatter', mode = 'lines')
```
:::
:::

## Next steps

The most difficult part of using EBASE is preparing the data.  Now that we have learned how to import and clean the data, we are ready for analysis. In the next lesson, we'll learn about the theory behind EBASE, how to use it to estimate metabolism, and how to interpret the results.